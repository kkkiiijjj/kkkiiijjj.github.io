<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>2023-12-8 | Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="单词： synthetic  合成 indices index的复数形式，意思是指标 shuffle：洗牌，打乱 entropy: 熵 flatten: 扁平化 Gradient：梯度  机器学习术语： label:  分类，比如猫，狗 feature:  比如四条腿、有胡子、有尾巴\ 样本 Example：样本是指一组或几组数据，样本一般作为训练数据来训练模型样本分为以下两类：有标签样本无标签样">
<meta property="og:type" content="article">
<meta property="og:title" content="2023-12-8">
<meta property="og:url" content="https://kkkiiijjj.github.io/2023/12/08/2023-12-8/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:description" content="单词： synthetic  合成 indices index的复数形式，意思是指标 shuffle：洗牌，打乱 entropy: 熵 flatten: 扁平化 Gradient：梯度  机器学习术语： label:  分类，比如猫，狗 feature:  比如四条腿、有胡子、有尾巴\ 样本 Example：样本是指一组或几组数据，样本一般作为训练数据来训练模型样本分为以下两类：有标签样本无标签样">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://kkkiiijjj.github.io/20210219103430955.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-14.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-15.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-16.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-17.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-18.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-19.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-20.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-21.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-22.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-23.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-40.png">
<meta property="og:image" content="https://kkkiiijjj.github.io/image-41.png">
<meta property="article:published_time" content="2023-12-08T00:57:27.000Z">
<meta property="article:modified_time" content="2024-03-04T13:29:23.198Z">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kkkiiijjj.github.io/20210219103430955.png">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2023-12-8" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/08/2023-12-8/" class="article-date">
  <time class="dt-published" datetime="2023-12-08T00:57:27.000Z" itemprop="datePublished">2023-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      2023-12-8
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="单词："><a href="#单词：" class="headerlink" title="单词："></a>单词：</h2><ul>
<li>synthetic  合成</li>
<li>indices index的复数形式，意思是指标</li>
<li>shuffle：洗牌，打乱</li>
<li>entropy: 熵</li>
<li>flatten: 扁平化</li>
<li>Gradient：梯度</li>
</ul>
<h2 id="机器学习术语："><a href="#机器学习术语：" class="headerlink" title="机器学习术语："></a>机器学习术语：</h2><ol>
<li><strong>label</strong>:  分类，比如猫，狗</li>
<li><strong>feature</strong>:  比如四条腿、有胡子、有尾巴\</li>
<li>样本 Example：<br>样本是指一组或几组数据，样本一般作为训练数据来训练模型<br>样本分为以下两类：<br>有标签样本<br>无标签样本\</li>
<li>模型 Model：模型定义了特征与标签之间的关系\</li>
<li>训练 Training是指创建或学习模型。也就是说，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。<br>训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值<br>推断 Inference是指将训练后的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测 (y’)。</li>
<li>回归 Regression  &#x2F;rɪˈɡreʃ.ən&#x2F;<br>就是根据数据集，建立一个线性方程组，能够无限逼近数据集中的数据点<br>回归模型可预测连续值（线性）<br>当机器学习模型最终目标（模型输出）是求一个具体数值时，例如房价的模型输出为25000，则大多数可以通过回归问题来解决。<br>线性回归的好处在与模型简单，计算速度快，方便应用在分布式系统对大数据进行处理。<br>线性回归还有个姐妹：逻辑回归（Logistic Regression），主要应用在分类领域</li>
<li>分类 Classification<br>顾名思义，分类模型可用来预测离散值<br>例如，分类模型做出的预测可回答如下问题：<br>是&#x2F;否问题，某个指定电子邮件是垃圾邮件还是非垃圾邮件？<br>图片是动物还是人？<br>垃圾分类<br>当机器学习模型最终目标（模型输出）是布尔或一定范围的数时，例如判断一张图片是不是人，模型输出0&#x2F;1：0不是，1是；又例如垃圾分类，模型输出1-10之间的整数，1代表生活垃圾，2代表厨余垃圾。。等等，这类需求则大多数可以通过分类问题来解决。<br><img src="/20210219103430955.png" alt="Alt text"></li>
</ol>
<p><strong>range</strong>(start, stop[, step])<br>这个我记得前面记过一次，再来一遍<br>实例</p>
<blockquote>
<blockquote>
<blockquote>
<p>range(0, 30, 5)  # 步长为 5<br>[0, 5, 10, 15, 20, 25]<br>range(0, 10, 3)  # 步长为 3<br>[0, 3, 6, 9]<br>range(0, -10, -1) # 负数<br>[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]<br>range(0)<br>[]<br>range(1, 0)<br>[]</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h2><p>numpy.array(object, dtype&#x3D;None, copy&#x3D;True, order&#x3D;’K’, subok&#x3D;False, ndmin&#x3D;0)<br>作用：创建一个数组<br>object:数组<br>dtype ： 数据类型，可选<br>ndmin ： int，可选  指定结果数组应具有的最小维数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([(1, 1000), (0, 1111)], dtype=[(&#x27;x&#x27;, &#x27;**&lt;i4**&#x27;), (&#x27;y&#x27;, &#x27;&lt;i4&#x27;)])</span><br></pre></td></tr></table></figure>
<p>这里的<strong>dtype</strong>的作用：给array中初始化值指定名字和数据类型。每一项初始化值的第一项被指定为’x’，第二项被指定为’y’。<br>分别使用名字索引访问x中的元素：<br>print(x[‘x’])  # 输出 [1 0]<br>print(x[‘y’])  # 输出 [1000 1111]</p>
<p><strong>yield</strong><br>如果不太好理解yield，可以先把yield当作return的同胞兄弟来看。<br>这两者的区别是：<br>有return的函数直接返回所有结果，程序终止不再运行，并销毁局部变量；<br>在调用生成器函数的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息（保留局部变量），返回yield的值, 并在下一次执行next()方法时从当前位置继续运行，直到生成器被全部遍历完。<br><img src="/image-14.png" alt="Alt text"><br><img src="/image-15.png" alt="Alt text"><br><img src="/image-16.png" alt="Alt text"><br><img src="/image-17.png" alt="Alt text"></p>
<p>**attach_grad()**函数：求梯度</p>
<h1 id="线性回归过程：生成数据集——读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练"><a href="#线性回归过程：生成数据集——读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练" class="headerlink" title="线性回归过程：生成数据集——读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练"></a>线性回归过程：生成数据集——读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练</h1><p>model——loss——optimizer</p>
<h2 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h2><p>待补充</p>
<h2 id="torchvision主要包括以下几个包：-vision-datasets-几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类-vision-models-流行的模型，例如-AlexNet-VGG-ResNet-和-Densenet-以及-与训练好的参数。-vision-transforms-常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor-numpy-数组到tensor-tensor-到-图像等。-vision-utils-用于把形似-3-x-H-x-W-的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"><a href="#torchvision主要包括以下几个包：-vision-datasets-几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类-vision-models-流行的模型，例如-AlexNet-VGG-ResNet-和-Densenet-以及-与训练好的参数。-vision-transforms-常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor-numpy-数组到tensor-tensor-到-图像等。-vision-utils-用于把形似-3-x-H-x-W-的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。" class="headerlink" title="torchvision主要包括以下几个包：* vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类* vision.models : 流行的模型，例如 AlexNet, VGG, ResNet 和 Densenet 以及 与训练好的参数。* vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor ,numpy 数组到tensor , tensor 到 图像等。* vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"></a>torchvision主要包括以下几个包：<br>* vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类<br>* vision.models : 流行的模型，例如 AlexNet, VGG, ResNet 和 Densenet 以及 与训练好的参数。<br>* vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor ,numpy 数组到tensor , tensor 到 图像等。<br>* vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。</h2><h2 id="torch-rand（）："><a href="#torch-rand（）：" class="headerlink" title="torch.rand（）："></a>torch.rand（）：</h2><p>张量生成函数（torch.rand的作用通俗来说就是产生均匀分布的数据！<br>torch.rand（）里面有几个数字那么就是生成几维张量！<br>生成一个有四个随机张量元素的一维张量：<br>x &#x3D; torch.rand(4)<br>结果：<br><img src="/image-18.png" alt="Alt text"><br>x &#x3D; torch.rand(2,4)<br>生成了个两行四列的张量（矩阵）：<br><img src="/image-19.png" alt="Alt text"><br>x &#x3D; torch.rand(2,3,4)<br><img src="/image-20.png" alt="Alt text"><br>这里通俗理解为，生成了2个平行摆放的3行4列的二维张量！</p>
<h2 id="flatten-函数："><a href="#flatten-函数：" class="headerlink" title="flatten()函数："></a>flatten()函数：</h2><p>flatten()是对多维数据的降维函数。flatten(),默认缺省参数为0，有参数则从第dim个维度开始展开<br>常用在从卷积层到全连接层的过渡<br>例：<br>a &#x3D; torch.rand(2,3,4)<br><img src="/image-21.png" alt="Alt text"><br>b &#x3D; a.flatten()<br><img src="/image-22.png" alt="Alt text"><br>d &#x3D; a.flatten(1)<br><img src="/image-23.png" alt="Alt text"><br>ps:这不就是去中括号吗</p>
<p>进一步了解flatten在cnn中的应用：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/495408190">https://zhuanlan.zhihu.com/p/495408190</a><br><strong>sum()</strong>:<br>语法：sum(iterable[, start])<br>iterable – 可迭代对象，如：列表、元组、集合。<br>start – 指定相加的参数，如果没有设置这个值，默认为0。<br>例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sum([0,1,2])  </span><br><span class="line">3  </span><br><span class="line">&gt;&gt;&gt; sum((2, 3, 4), 1)        # 元组计算总和后再加 1\</span><br><span class="line">10\</span><br><span class="line">&gt;&gt;&gt; sum([0,1,2,3,4], 2)      # 列表计算总和后再加 2\</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<h2 id="torch-sum-："><a href="#torch-sum-：" class="headerlink" title="torch.sum()："></a>torch.sum()：</h2><p>torch.sum(input, dtype&#x3D;None)<br>input:输入一个tensor<br>dim:要求和的维度，可以是一个列表<br>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones((2, 3))</span><br><span class="line">print(a):</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line"> 		[1, 1, 1]])</span><br><span class="line"></span><br><span class="line">a1 =  torch.sum(a)</span><br><span class="line">a2 =  torch.sum(a, dim=0)</span><br><span class="line">a3 =  torch.sum(a, dim=1)</span><br><span class="line">print(a)</span><br><span class="line">print(a1)</span><br><span class="line">print(a2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(6.)</span><br><span class="line">tensor([2., 2., 2.])</span><br><span class="line">tensor([3., 3.])</span><br></pre></td></tr></table></figure>
<p>如果加上keepdim&#x3D;True, 则会保持dim的维度不被squeeze:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1 =  torch.sum(a, dim=(0, 1), keepdim=True)</span><br><span class="line">a2 =  torch.sum(a, dim=(0, ), keepdim=True)</span><br><span class="line">a3 =  torch.sum(a, dim=(1, ), keepdim=True)</span><br><span class="line">输出结果：</span><br><span class="line">tensor([[6.]])</span><br><span class="line">tensor([[2., 2., 2.]])</span><br><span class="line">tensor([[3., 3.]])</span><br></pre></td></tr></table></figure>
<p><strong>dim</strong>&#x3D;0压缩列，dim&#x3D;1压缩行<br>维度是从0开始索引的，矩阵有两个维度，第一个是0维度，第二个是1维度，其中0维度对应行，1维度对应列<br>一维数组一定要用 1 × n 来记！！</p>
<p><strong>axis</strong>:(轴)<br>看这篇文章吧：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/sky_kkk/article/details/79725646">https://blog.csdn.net/sky_kkk/article/details/79725646</a></p>
<p>**shape[0]**代表行数，shape[1]代表列数，同理三维张量还有shape[2]<br>-1代表最后一个，所以shape[-1]代表最后一个维度</p>
<p><strong>len()</strong>:<br>返回Tensor的第0维长度，将样本横向堆叠的时候可以返回样本的数量。</p>
<p><strong>numpy.argmax(array, axis)</strong> 用于返回一个numpy数组中最大值的索引值。当一组中同时出现几个最大值时，返回第一个</p>
<p>常用于得到概率最大的结果</p>
<p>在运算时，相当于剥掉一层中括号，返回一个数组，分为一维和多维。一维数组剥掉一层中括号之后就成了一个索引值，是一个数，而n维数组剥掉一层中括号后，会返回一个 n-1 维数组，而剥掉哪一层中括号，取决于axis的取值。<br>array &#x3D; np.array([[1, 3, 5], [0, 4, 3]])<br>axis0 &#x3D; np.argmax(array, axis &#x3D; 0)<br>axis1 &#x3D; np.argmax(array, axis &#x3D; 1)<br>print(axis0)<br>print(axis1)<br>输出结果<br>[0 1 0]<br>[2 1]</p>
<p>three_dim_array &#x3D; [[[1, 2, 3, 4],  [-1, 0, 3, 5]],<br>                   [[2, 7, -1, 3], [0, 3, 12, 4]],<br>                   [[5, 1, 0, 19], [4, 2, -2, 13]]]<br>a &#x3D; np.argmax(three_dim_array, axis &#x3D; 0)<br>print(a)<br>b &#x3D; np.argmax(three_dim_array, axis &#x3D; 1)<br>print(b)<br>c &#x3D; np.argmax(test, axis &#x3D; 2)<br>print(c)<br>[[2 1 0 2]<br> [2 1 1 2]]</p>
<p>[[0 0 0 1]<br> [0 0 1 1]<br> [0 1 0 0]]</p>
<p>[[3 3]<br> [1 2]<br> [3 3]] </p>
<p> ps:这什么鬼东西</p>
<p> <strong>isinstance()</strong> 函数：<br> 来判断一个对象是否是一个已知的类型<br> 语法形式为：isinstance(object, classinfo)<br> object – 实例对象<br> classinfo – 数据类型<br> 返回值：相同则返回 True，否则返回 False。<br>isinstance()函数和type()函数很类似。但是type() 不考虑继承关系。isinstance() 考虑继承关系。<br>如果要判断两个类型是否相同推荐使用 isinstance()。<br>这里给出两个函数区别的示例：<br>class A:<br>    pass<br>class B(A):<br>    pass<br>isinstance(A(), A)    # returns True<br>type(A()) &#x3D;&#x3D; A        # returns True<br>isinstance(B(), A)    # returns True<br>type(B()) &#x3D;&#x3D; A        # returns False</p>
<h2 id="with-torch-no-grad"><a href="#with-torch-no-grad" class="headerlink" title="with torch.no_grad():"></a>with torch.no_grad():</h2><p>非常常见。<br>总结with工作原理：<br>（１）紧跟with后面的语句被求值后，返回对象的“–enter–()”方法被调用，这个方法的返回值将被赋值给as后面的变量；<br>（２）当with后面的代码块全部被执行完之后，将调用前面返回对象的“–exit–()”方法。<br>当requires_grad设置为False时,反向传播时就不会自动求导了，因此大大节约了显存或者说内存。<br>with torch.no_grad的作用<br>在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。<br>即使一个tensor（命名为x）的requires_grad &#x3D; True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。</p>
<h2 id="重要"><a href="#重要" class="headerlink" title="重要"></a>重要</h2><p>在用pytorch训练模型时，通常会在遍历epochs的过程中依次用到<strong>optimizer.zero_grad()</strong> ，<strong>loss.backward()</strong> ，<strong>optimizer.step()</strong> 三个函数<br>这三个函数的作用是先将梯度归零（optimizer.zero_grad()），然后反向传播计算得到每个参数的梯度值（loss.backward()），最后通过梯度下降执行一步参数更新（optimizer.step()）<br>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后<br>使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</p>
<p>忽然发现我把sigmoid()和softmax()函数弄混了<br>他们都是激活函数,但公式不一样<br><img src="/image-40.png" alt="Alt text">用于多类分类问题<br><img src="/image-41.png" alt="Alt text">(sigmoid)</p>
<p>激活函数：ReLU,sigmoid,tanh<br>其中ReLU用的更多(因为计算简单)</p>
<p>通常，我们选择2的若干次幂作为层的宽度</p>
<p><strong>torch.nn.Parameter</strong>继承torch.Tensor，其作用将一个不可训练的类型为Tensor的参数转化为可训练的类型为parameter的参数，并将这个参数绑定到module里面，成为module中可训练的参数。（没看懂）</p>
<p><strong>torch.randn()</strong> 返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。<br>torch.randn(*size, *, out&#x3D;None, dtype&#x3D;None, layout&#x3D;torch.strided, device&#x3D;None, requires_grad&#x3D;False) → Tensor<br>out(Tensor, optional) –输出张量<br>dtype(torch.dtype, optional) –返回张量所需的数据类型。默认:如果没有，使用全局默认值<br>layout(torch.layout, optional) –返回张量的期望布局。默认值:torch.strided<br>device(torch.device, optional) –返回张量的所需 device。默认:如果没有，则使用当前设备作为默认张量类型.(CPU或CUDA)<br>requires_grad(bool, optional) –autograd是否应该记录对返回张量的操作(说明当前量是否需要在计算中保留对应的梯度信息)。默认值:False。</p>
<p>用于对抗过拟合的技术称为正则化（regularization）。<br>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制</p>
<p>通常将我们的数据分成三份， 训练、测试、验证数据集（validation dataset）， 也叫验证集（validation set）。<br><strong>验证集</strong>（validation set）：依靠它来进行模型的选择</p>
<p><strong>K折交叉验证</strong><br>当训练数据稀缺时，将原始训练数据分成K个不重叠的子集。 然后执行K次模型训练和验证，每次在K-1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对K次实验的结果取平均来估计训练和验证误差</p>
<p><strong>power(x, y)</strong> 函数，计算 x 的 y 次方。</p>
<p><strong>np.arange()</strong> 函数<br>语法：<br>numpy.arange(start, stop, step, dtype)<br>start	起始值，默认为0(可不写)<br>stop	终止值（不包含）<br>step	步长，默认为1(可不写)<br>dtype	返回ndarray的数据类型，如果没有提供，则会使用输入数据的类型。</p>
<p><strong>numpy.arange(n).reshape(a, b)</strong>   依次生成n个自然数，并且以a行b列的数组形式显示</p>
<p><strong>numel</strong>函数<br>获取tensor中一共包含多少个元素</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/08/2023-12-8/" data-id="cltl8pnbv00012ot03zz4ffq5" data-title="2023-12-8" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/12/09/2023-12-10/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          2023-12-9
        
      </div>
    </a>
  
  
    <a href="/2023/12/06/2023-12-6/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">2023-12-6</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/04/19/RNN/">RNN</a>
          </li>
        
          <li>
            <a href="/2024/04/17/Kaggle-LLM-Science-Exam/">Kaggle - LLM Science Exam</a>
          </li>
        
          <li>
            <a href="/2024/04/17/kaggle%E2%80%94HMS/">kaggle—HMS</a>
          </li>
        
          <li>
            <a href="/2024/04/13/prompt/">prompt</a>
          </li>
        
          <li>
            <a href="/2024/04/08/LSTM/">LSTM</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>