<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Forforevery">
<meta property="og:url" content="https://kkkiiijjj.github.io/page/3/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-字符编码" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/09/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/" class="article-date">
  <time class="dt-published" datetime="2024-03-09T03:19:36.000Z" itemprop="datePublished">2024-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/09/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/">字符编码</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>今天在练习pandas库中途遇到了一道题，从CSV中创建 DataFrame，分隔符为“；”，编码格式为gbk<br>答案是<br>df &#x3D; pd.read_csv(‘test.csv’, encoding&#x3D;’gbk, sep&#x3D;’;’)<br>然后不懂这个编码格式，也不知道有没有必要了解一下，先开个坑，以后回来填</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/09/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/" data-id="cltl8pnc2000b2ot01hgxh6rh" data-title="字符编码" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据预处理" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/04/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" class="article-date">
  <time class="dt-published" datetime="2024-03-04T12:22:19.000Z" itemprop="datePublished">2024-03-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/04/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/">数据预处理</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>数据预处理包括数据的清洗、集成、变换</p>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109413107">https://zhuanlan.zhihu.com/p/109413107</a><br>删除无关数据、重复数据、平滑噪声数据，处理缺失值、异常值等。<br>一般进行数据清洗需要通过通过7个步骤进行处理： 选择子集，列名重命名，删除重复值，缺失值处理，一致化处理，数据排序处理，异常值处理</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>拉格朗日插值法、牛顿插值法<br><img src="/image-74.png" alt="alt text"></p>
<h3 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h3><p><img src="/image-73.png" alt="alt text"></p>
<h2 id="数据变换"><a href="#数据变换" class="headerlink" title="数据变换"></a>数据变换</h2><h3 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h3><p>a.</p>
<h1 id="序列数据的数据清洗"><a href="#序列数据的数据清洗" class="headerlink" title="序列数据的数据清洗"></a>序列数据的数据清洗</h1><p>经常要进行的如</p>
<ol>
<li>大写字母转小写或小写转大写</li>
<li>去掉多余的whitespaces,如换行符、tab、非间断空格</li>
<li>去掉没用的符号</li>
<li>去掉链接</li>
<li>Expanding abbreviations扩充缩写</li>
<li>纠正拼写错误</li>
<li>词元化(tokenization)</li>
<li>去掉stop word如the,a,is(可选)<br>具体情况具体分析</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/04/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" data-id="cltl8pnc3000d2ot035ni85ch" data-title="数据预处理" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据集划分" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/01/07/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/" class="article-date">
  <time class="dt-published" datetime="2024-01-07T03:26:06.000Z" itemprop="datePublished">2024-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/01/07/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/">数据集划分</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>下载后的数据集类型：</p>
<p><img src="/image-72.png" alt="Alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/01/07/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/" data-id="cltl8pnc3000c2ot0degbbs5y" data-title="数据集划分" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-torch-nn-init" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/27/torch-nn-init/" class="article-date">
  <time class="dt-published" datetime="2023-12-27T01:46:52.000Z" itemprop="datePublished">2023-12-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/27/torch-nn-init/">torch.nn.init</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li>torch.nn.init.uniform_                </li>
<li>torch.nn.init.normal_</li>
<li>torch.nn.init.constant_</li>
<li>torch.nn.init.ones_</li>
<li>torch.nn.init.zeros_</li>
<li>torch.nn.init.xavier_uniform_</li>
<li>torch.nn.init.xavier_normal_<br>下面这些暂时未接触</li>
<li>torch.nn.init.calculate_gain</li>
<li>torch.nn.init.eye_</li>
<li>torch.nn.init.dirac_</li>
<li>torch.nn.init.kaiming_normal_</li>
<li>torch.nn.init.trunc_normal_</li>
<li>torch.nn.init.orthogonal_</li>
<li>torch.nn.init.sparse_</li>
</ul>
<h3 id="torch-nn-init-uniform"><a href="#torch-nn-init-uniform" class="headerlink" title="torch.nn.init.uniform_"></a>torch.nn.init.uniform_</h3><p>从均匀分布U ( a , b ) 中生成值<br>语法<br>torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0)<br>a:下界<br>b:上界</p>
<h3 id="torch-nn-init-normal"><a href="#torch-nn-init-normal" class="headerlink" title="torch.nn.init.normal_"></a>torch.nn.init.normal_</h3><p>给定均值和标准差的正态分布N ( mean , std ) N(\text{mean}, \text{std})N(mean,std)中生成值<br>语法<br>torch.nn.init.normal_(tensor, mean&#x3D;0.0, std&#x3D;1.0)</p>
<h3 id="torch-nn-init-constant"><a href="#torch-nn-init-constant" class="headerlink" title="torch.nn.init.constant_"></a>torch.nn.init.constant_</h3><p>用val的值填充<br>语法<br>torch.nn.init.constant_(tensor, val)</p>
<h3 id="torch-nn-init-xavier-uniform"><a href="#torch-nn-init-xavier-uniform" class="headerlink" title="torch.nn.init.xavier_uniform_"></a>torch.nn.init.xavier_uniform_</h3><p>张量中的值采样自U ( − a , a )，其中：<br><img src="/image-70.png" alt="Alt text"><br>语法<br>torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1)<br>gain ：[float] 可选的缩放因子</p>
<h3 id="torch-nn-init-xavier-normal"><a href="#torch-nn-init-xavier-normal" class="headerlink" title="torch.nn.init.xavier_normal_"></a>torch.nn.init.xavier_normal_</h3><p>结果张量中的值采样自N(0, ${std}^2$)的正态分布，其中标准差：<img src="/image-71.png" alt="Alt text"><br>语法<br>torch.nn.init.xavier_normal_(tensor, gain&#x3D;1.0)<br>gain ：[float] 可选的缩放因子</p>
<p>节选自：<br><a target="_blank" rel="noopener" href="https://machinelearning.blog.csdn.net/article/details/132382885">https://machinelearning.blog.csdn.net/article/details/132382885</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/27/torch-nn-init/" data-id="cltl8pnc2000a2ot053zx31sc" data-title="torch.nn.init" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/23/CNN/" class="article-date">
  <time class="dt-published" datetime="2023-12-23T02:01:21.000Z" itemprop="datePublished">2023-12-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/23/CNN/">CNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  CNN是识别图像最佳学习算法之一，并且在图像分割、分类、检测和检索相关任务中表现出色</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/image-52.png" alt="Alt text"><br><img src="/image-53.png" alt="Alt text"></p>
<h2 id="卷积网络的层级结构"><a href="#卷积网络的层级结构" class="headerlink" title="卷积网络的层级结构"></a>卷积网络的层级结构</h2><p>CNN的结构可以分为3层：</p>
<ul>
<li>卷积层(Convolutional Layer) - 主要作用是提取特征。</li>
<li>池化层(Max Pooling Layer) - 主要作用是下采样(downsampling)，却不会损坏识别结果。</li>
<li>全连接层(Fully Connected Layer) - 主要作用是分类。</li>
</ul>
<h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p><img src="/image-54.png" alt="Alt text"></p>
<p>插个嘴，论文推荐：Rectified Linear Units Improve Restricted Boltzmann Machines</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><img src="/%E5%8D%B7%E7%A7%AF.gif" alt="Alt text"></p>
<h3 id="池化层-汇聚层"><a href="#池化层-汇聚层" class="headerlink" title="池化层(汇聚层)"></a>池化层(汇聚层)</h3><p>压缩图像，提取特征，防止过拟合<br>作用：<br>降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。<br>汇聚层的输出通道数与输入通道数相同<br><strong>Max pooling</strong><br><img src="/image-55.png" alt="Alt text"></p>
<hr>
<p>一些说明</p>
<ol>
<li>CNN中用的优化算法是SGD（随机梯度下降）。</li>
<li>优缺点</li>
</ol>
<p>（1）优点<br>　　•共享卷积核，对高维数据处理无压力<br>　　•无需手动选取特征，训练好权重，即得特征分类效果好<br>（2）缺点<br>　　•需要调参，需要大样本量，训练最好要GPU<br>　　•物理含义不明确（也就说，我们并不知道没个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”）</p>
<ol start="3">
<li>典型CNN模型<br>LeNet，这是最早用于数字识别的CNN<br>AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比<br>LeNet更深，用多层小卷积层叠加替换单大卷积层。<br>ZF Net， 2013 ILSVRC比赛冠军<br>GoogLeNet， 2014 ILSVRC比赛冠军<br>VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好</li>
</ol>
<h2 id="conv2d-的参数"><a href="#conv2d-的参数" class="headerlink" title="conv2d 的参数"></a>conv2d 的参数</h2><p> in_channels （int） – 输入图像中的通道数</p>
<p> out_channels （int） – 卷积产生的通道数</p>
<p> kernel_size （int or tuple） - 卷积内核的大小</p>
<p> stride （int 或 tuple， 可选） - 卷积的步幅。默认值：1</p>
<p> padding （int， tuple or str， 可选） – 添加到 输入。默认值：0</p>
<h3 id="in-channels"><a href="#in-channels" class="headerlink" title="in_channels:"></a>in_channels:</h3><p>最初为输入图片的  颜色层数  黑白的是1，RGB的是3<br>往后的 看上一层的out_channels</p>
<h3 id="out-channels"><a href="#out-channels" class="headerlink" title="out_channels:"></a>out_channels:</h3><p><img src="/image-57.png" alt="Alt text"><br>out_channels 为 1<br><img src="/image-58.png" alt="Alt text"><br>out_channels 为 2<br>所以out_channels是多少看卷积核有几个<br>但是。。。卷积核设置成多少竟然是凭感觉？？？<br>kernal_size:卷积核越小，所需要的参数和计算量越小。所以2×2排除了，一般都用3×3作为卷积核的大小。</p>
<hr>
<p>不管输入图像的深度为多少，经过一个卷积核（filter），最后都变成一个深度为1的特征图。<br>卷积核(filter)的channel数，其实是与输入图像相同的，所以常常会省略不写，只写输出filters数量</p>
<h2 id="MaxPool2d参数"><a href="#MaxPool2d参数" class="headerlink" title="MaxPool2d参数"></a>MaxPool2d参数</h2><p>kernel_size ：表示做最大池化的窗口大小，可以是单个值，也可以是tuple元组</p>
<p>stride ：步长，可以是单个值，也可以是tuple元组</p>
<p>padding ：填充，可以是单个值，也可以是tuple元组</p>
<p>kernel_size 的详解<br>注意这里的 kernel_size 跟卷积核不是一个东西。 kernel_size 可以看做是一个滑动窗口，这个窗口的大小由自己指定，如果输入是单个值，例如 3 ，那么窗口的大小就是 3 × 3 3 ，还可以输入元组，例如 (3, 2) ，那么窗口大小就是 3 × 2。<br>最大池化的方法就是取这个窗口覆盖元素中的最大值</p>
<p>stride 的详解<br>上一个参数我们确定了滑动窗口的大小，现在我们来确定这个窗口如何进行滑动。如果不指定这个参数，那么默认步长跟最大池化窗口大小一致。如果指定了参数，那么将按照我们指定的参数进行滑动。例如 stride&#x3D;(2,3) ， 那么窗口将每次向右滑动三个元素位置，或者向下滑动两个元素位置</p>
<h2 id="nn-Linear（）参数"><a href="#nn-Linear（）参数" class="headerlink" title="nn.Linear（）参数"></a>nn.Linear（）参数</h2><p>in_features指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。<br>  out_features指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。<br>  从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。<br>其实参数就是权重矩阵的行列数</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/23/CNN/" data-id="cltl8pnbw00022ot0fdrx51ob" data-title="CNN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CNN-RNN-LSTM-Transformer" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/21/CNN-RNN-LSTM-Transformer/" class="article-date">
  <time class="dt-published" datetime="2023-12-21T11:16:56.000Z" itemprop="datePublished">2023-12-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/21/CNN-RNN-LSTM-Transformer/">CNN RNN LSTM Transformer</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <table>
<thead>
<tr>
<th></th>
<th>RNN</th>
<th>LSTM</th>
<th>CNN</th>
<th>Transformer</th>
</tr>
</thead>
<tbody><tr>
<td>用于</td>
<td>NLP</td>
<td>NLP</td>
<td>CV,NLP</td>
<td>CV,NLP</td>
</tr>
<tr>
<td>优点</td>
<td>1.有效地捕捉序列中的时序信息2.共享权重3.上下文依赖</td>
<td>1.长期依赖建模2.防止梯度消失和梯度爆炸(利用门控)3.灵活的门控结构</td>
<td>1.共享卷积核</td>
<td>1.突破了RNN模型不能并行计算的限制2.比RNN更快，比CNN更深</td>
</tr>
<tr>
<td>缺点</td>
<td>1.梯度消失&#x2F;爆炸(在反向传播时，由于参数共享和多次连乘的特性)2.难以捕捉到长期依赖关系3.计算效率较低4.难以并行</td>
<td>1.计算复杂度较高2参教量较多</td>
<td>1.当网络层数太深时，采用反向传播调整内部参数会导致梯度消失2.池化层会丢失一定的有价值信息，忽略了局部与整体之间的关联性3.远距离的特征捕获能力相对较弱</td>
<td>1.局部信息的获取不如RNN和CNN强2.并不能很好表征位置信息3.如果层数较多时连乘计算会使得顶层出现梯度消失问题</td>
</tr>
</tbody></table>
<p>transformer目前在各方面来说都是较优的<br><img src="/image-46.png" alt="Alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/21/CNN-RNN-LSTM-Transformer/" data-id="cltl8pnby00042ot02pilcya6" data-title="CNN RNN LSTM Transformer" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2023-12-10" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/09/2023-12-10/" class="article-date">
  <time class="dt-published" datetime="2023-12-09T13:10:32.000Z" itemprop="datePublished">2023-12-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/09/2023-12-10/">2023-12-9</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>ImageFolder(root, transform&#x3D;None, target_transform&#x3D;None, loader&#x3D;default_loader)<br>root：在root指定的路径下寻找图片<br>transform：对PIL Image进行的转换操作，transform的输入是使用loader读取图片的返回对象<br>target_transform：对label的转换<br>loader：给定路径后如何读取图片，默认读取为RGB格式的PIL Image对象</p>
<h2 id="eval-函数"><a href="#eval-函数" class="headerlink" title="eval() 函数"></a>eval() 函数</h2><p>用来执行一个字符串表达式，并返回表达式的值<br>expression – 表达式。<br>globals – 变量作用域，全局命名空间，如果被提供，则必须是一个字典对象。<br>locals – 变量作用域，局部命名空间，如果被提供，可以是任何映射对象。<br>实例 1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;x = 7</span><br><span class="line">&gt;&gt;&gt; eval( &#x27;3 * x&#x27; )</span><br><span class="line">21</span><br><span class="line">&gt;&gt;&gt; eval(&#x27;pow(2,2)&#x27;)</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt; eval(&#x27;2 + 2&#x27;)</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt; n=81</span><br><span class="line">&gt;&gt;&gt; eval(&quot;n + 4&quot;)</span><br><span class="line">85</span><br></pre></td></tr></table></figure>
<p>实例 2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">## 执行简单的数学表达式</span><br><span class="line">result = eval(&quot;2 + 3 * 4&quot;)</span><br><span class="line">print(result)  # 输出: 14</span><br><span class="line"></span><br><span class="line">## 执行变量引用</span><br><span class="line">x = 10</span><br><span class="line">result = eval(&quot;x + 5&quot;)</span><br><span class="line">print(result)  # 输出: 15</span><br><span class="line"></span><br><span class="line">## 在指定命名空间中执行表达式</span><br><span class="line">namespace = &#123;&#x27;a&#x27;: 2, &#x27;b&#x27;: 3&#125;</span><br><span class="line">result = eval(&quot;a + b&quot;, namespace)</span><br><span class="line">print(result)  # 输出: 5</span><br></pre></td></tr></table></figure>
<p>eval()使用原因：</p>
<p>1）在编译语言里要动态地产生代码，基本上是不可能的，但动态语言是可以，意味着软件已经部署到服务器上了，但只要作很少的更改，只好直接修改这部分的代码，就可立即实现变化，不用整个软件重新加载。</p>
<p>2）在machin learning里根据用户使用这个软件频率，以及方式，可动态地修改代码，适应用户的变化</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/09/2023-12-10/" data-id="cltl8pnbo00002ot05vg549y5" data-title="2023-12-9" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2023-12-8" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/08/2023-12-8/" class="article-date">
  <time class="dt-published" datetime="2023-12-08T00:57:27.000Z" itemprop="datePublished">2023-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/08/2023-12-8/">2023-12-8</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="单词："><a href="#单词：" class="headerlink" title="单词："></a>单词：</h2><ul>
<li>synthetic  合成</li>
<li>indices index的复数形式，意思是指标</li>
<li>shuffle：洗牌，打乱</li>
<li>entropy: 熵</li>
<li>flatten: 扁平化</li>
<li>Gradient：梯度</li>
</ul>
<h2 id="机器学习术语："><a href="#机器学习术语：" class="headerlink" title="机器学习术语："></a>机器学习术语：</h2><ol>
<li><strong>label</strong>:  分类，比如猫，狗</li>
<li><strong>feature</strong>:  比如四条腿、有胡子、有尾巴\</li>
<li>样本 Example：<br>样本是指一组或几组数据，样本一般作为训练数据来训练模型<br>样本分为以下两类：<br>有标签样本<br>无标签样本\</li>
<li>模型 Model：模型定义了特征与标签之间的关系\</li>
<li>训练 Training是指创建或学习模型。也就是说，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。<br>训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值<br>推断 Inference是指将训练后的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测 (y’)。</li>
<li>回归 Regression  &#x2F;rɪˈɡreʃ.ən&#x2F;<br>就是根据数据集，建立一个线性方程组，能够无限逼近数据集中的数据点<br>回归模型可预测连续值（线性）<br>当机器学习模型最终目标（模型输出）是求一个具体数值时，例如房价的模型输出为25000，则大多数可以通过回归问题来解决。<br>线性回归的好处在与模型简单，计算速度快，方便应用在分布式系统对大数据进行处理。<br>线性回归还有个姐妹：逻辑回归（Logistic Regression），主要应用在分类领域</li>
<li>分类 Classification<br>顾名思义，分类模型可用来预测离散值<br>例如，分类模型做出的预测可回答如下问题：<br>是&#x2F;否问题，某个指定电子邮件是垃圾邮件还是非垃圾邮件？<br>图片是动物还是人？<br>垃圾分类<br>当机器学习模型最终目标（模型输出）是布尔或一定范围的数时，例如判断一张图片是不是人，模型输出0&#x2F;1：0不是，1是；又例如垃圾分类，模型输出1-10之间的整数，1代表生活垃圾，2代表厨余垃圾。。等等，这类需求则大多数可以通过分类问题来解决。<br><img src="/20210219103430955.png" alt="Alt text"></li>
</ol>
<p><strong>range</strong>(start, stop[, step])<br>这个我记得前面记过一次，再来一遍<br>实例</p>
<blockquote>
<blockquote>
<blockquote>
<p>range(0, 30, 5)  # 步长为 5<br>[0, 5, 10, 15, 20, 25]<br>range(0, 10, 3)  # 步长为 3<br>[0, 3, 6, 9]<br>range(0, -10, -1) # 负数<br>[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]<br>range(0)<br>[]<br>range(1, 0)<br>[]</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h2><p>numpy.array(object, dtype&#x3D;None, copy&#x3D;True, order&#x3D;’K’, subok&#x3D;False, ndmin&#x3D;0)<br>作用：创建一个数组<br>object:数组<br>dtype ： 数据类型，可选<br>ndmin ： int，可选  指定结果数组应具有的最小维数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([(1, 1000), (0, 1111)], dtype=[(&#x27;x&#x27;, &#x27;**&lt;i4**&#x27;), (&#x27;y&#x27;, &#x27;&lt;i4&#x27;)])</span><br></pre></td></tr></table></figure>
<p>这里的<strong>dtype</strong>的作用：给array中初始化值指定名字和数据类型。每一项初始化值的第一项被指定为’x’，第二项被指定为’y’。<br>分别使用名字索引访问x中的元素：<br>print(x[‘x’])  # 输出 [1 0]<br>print(x[‘y’])  # 输出 [1000 1111]</p>
<p><strong>yield</strong><br>如果不太好理解yield，可以先把yield当作return的同胞兄弟来看。<br>这两者的区别是：<br>有return的函数直接返回所有结果，程序终止不再运行，并销毁局部变量；<br>在调用生成器函数的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息（保留局部变量），返回yield的值, 并在下一次执行next()方法时从当前位置继续运行，直到生成器被全部遍历完。<br><img src="/image-14.png" alt="Alt text"><br><img src="/image-15.png" alt="Alt text"><br><img src="/image-16.png" alt="Alt text"><br><img src="/image-17.png" alt="Alt text"></p>
<p>**attach_grad()**函数：求梯度</p>
<h1 id="线性回归过程：-生成数据集——-读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练"><a href="#线性回归过程：-生成数据集——-读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练" class="headerlink" title="线性回归过程：(生成数据集——)读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练"></a>线性回归过程：(生成数据集——)读取数据集——数据预处理——初始化模型参数——定义模型——定义损失函数——定义优化算法——训练</h1><p>model——loss——optimizer</p>
<h2 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h2><p>待补充</p>
<h2 id="torchvision主要包括以下几个包：-vision-datasets-几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类-vision-models-流行的模型，例如-AlexNet-VGG-ResNet-和-Densenet-以及-与训练好的参数。-vision-transforms-常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor-numpy-数组到tensor-tensor-到-图像等。-vision-utils-用于把形似-3-x-H-x-W-的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"><a href="#torchvision主要包括以下几个包：-vision-datasets-几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类-vision-models-流行的模型，例如-AlexNet-VGG-ResNet-和-Densenet-以及-与训练好的参数。-vision-transforms-常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor-numpy-数组到tensor-tensor-到-图像等。-vision-utils-用于把形似-3-x-H-x-W-的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。" class="headerlink" title="torchvision主要包括以下几个包：* vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类* vision.models : 流行的模型，例如 AlexNet, VGG, ResNet 和 Densenet 以及 与训练好的参数。* vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor ,numpy 数组到tensor , tensor 到 图像等。* vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"></a>torchvision主要包括以下几个包：<br>* vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类<br>* vision.models : 流行的模型，例如 AlexNet, VGG, ResNet 和 Densenet 以及 与训练好的参数。<br>* vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor ,numpy 数组到tensor , tensor 到 图像等。<br>* vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。</h2><h2 id="torch-rand（）："><a href="#torch-rand（）：" class="headerlink" title="torch.rand（）："></a>torch.rand（）：</h2><p>张量生成函数（torch.rand的作用通俗来说就是产生均匀分布的数据！<br>torch.rand（）里面有几个数字那么就是生成几维张量！<br>生成一个有四个随机张量元素的一维张量：<br>x &#x3D; torch.rand(4)<br>结果：<br><img src="/image-18.png" alt="Alt text"><br>x &#x3D; torch.rand(2,4)<br>生成了个两行四列的张量（矩阵）：<br><img src="/image-19.png" alt="Alt text"><br>x &#x3D; torch.rand(2,3,4)<br><img src="/image-20.png" alt="Alt text"><br>这里通俗理解为，生成了2个平行摆放的3行4列的二维张量！</p>
<h2 id="flatten-函数："><a href="#flatten-函数：" class="headerlink" title="flatten()函数："></a>flatten()函数：</h2><p>flatten()是对多维数据的降维函数。flatten(),默认缺省参数为0，有参数则从第dim个维度开始展开<br>常用在从卷积层到全连接层的过渡<br>例：<br>a &#x3D; torch.rand(2,3,4)<br><img src="/image-21.png" alt="Alt text"><br>b &#x3D; a.flatten()<br><img src="/image-22.png" alt="Alt text"><br>d &#x3D; a.flatten(1)<br><img src="/image-23.png" alt="Alt text"><br>ps:这不就是去中括号吗</p>
<p>进一步了解flatten在cnn中的应用：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/495408190">https://zhuanlan.zhihu.com/p/495408190</a><br><strong>sum()</strong>:<br>语法：sum(iterable[, start])<br>iterable – 可迭代对象，如：列表、元组、集合。<br>start – 指定相加的参数，如果没有设置这个值，默认为0。<br>例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;sum([0,1,2])  </span><br><span class="line">3  </span><br><span class="line">&gt;&gt;&gt; sum((2, 3, 4), 1)        # 元组计算总和后再加 1\</span><br><span class="line">10\</span><br><span class="line">&gt;&gt;&gt; sum([0,1,2,3,4], 2)      # 列表计算总和后再加 2\</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<h2 id="torch-sum-："><a href="#torch-sum-：" class="headerlink" title="torch.sum()："></a>torch.sum()：</h2><p>torch.sum(input, dtype&#x3D;None)<br>input:输入一个tensor<br>dim:要求和的维度，可以是一个列表<br>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones((2, 3))</span><br><span class="line">print(a):</span><br><span class="line">tensor([[1, 1, 1],</span><br><span class="line"> 		[1, 1, 1]])</span><br><span class="line"></span><br><span class="line">a1 =  torch.sum(a)</span><br><span class="line">a2 =  torch.sum(a, dim=0)</span><br><span class="line">a3 =  torch.sum(a, dim=1)</span><br><span class="line">print(a)</span><br><span class="line">print(a1)</span><br><span class="line">print(a2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(6.)</span><br><span class="line">tensor([2., 2., 2.])</span><br><span class="line">tensor([3., 3.])</span><br></pre></td></tr></table></figure>
<p>如果加上keepdim&#x3D;True, 则会保持dim的维度不被squeeze:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1 =  torch.sum(a, dim=(0, 1), keepdim=True)</span><br><span class="line">a2 =  torch.sum(a, dim=(0, ), keepdim=True)</span><br><span class="line">a3 =  torch.sum(a, dim=(1, ), keepdim=True)</span><br><span class="line">输出结果：</span><br><span class="line">tensor([[6.]])</span><br><span class="line">tensor([[2., 2., 2.]])</span><br><span class="line">tensor([[3., 3.]])</span><br></pre></td></tr></table></figure>
<p><strong>dim</strong>&#x3D;0压缩列，dim&#x3D;1压缩行<br>维度是从0开始索引的，矩阵有两个维度，第一个是0维度，第二个是1维度，其中0维度对应行，1维度对应列<br>一维数组一定要用 1 × n 来记！！</p>
<p><strong>axis</strong>:(轴)<br>看这篇文章吧：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/sky_kkk/article/details/79725646">https://blog.csdn.net/sky_kkk/article/details/79725646</a></p>
<p>**shape[0]**代表行数，shape[1]代表列数，同理三维张量还有shape[2]<br>-1代表最后一个，所以shape[-1]代表最后一个维度</p>
<p><strong>len()</strong>:<br>返回Tensor的第0维长度，将样本横向堆叠的时候可以返回样本的数量。</p>
<p><strong>numpy.argmax(array, axis)</strong> 用于返回一个numpy数组中最大值的索引值。当一组中同时出现几个最大值时，返回第一个</p>
<p>常用于得到概率最大的结果</p>
<p>在运算时，相当于剥掉一层中括号，返回一个数组，分为一维和多维。一维数组剥掉一层中括号之后就成了一个索引值，是一个数，而n维数组剥掉一层中括号后，会返回一个 n-1 维数组，而剥掉哪一层中括号，取决于axis的取值。<br>array &#x3D; np.array([[1, 3, 5], [0, 4, 3]])<br>axis0 &#x3D; np.argmax(array, axis &#x3D; 0)<br>axis1 &#x3D; np.argmax(array, axis &#x3D; 1)<br>print(axis0)<br>print(axis1)<br>输出结果<br>[0 1 0]<br>[2 1]</p>
<p>three_dim_array &#x3D; [[[1, 2, 3, 4],  [-1, 0, 3, 5]],<br>                   [[2, 7, -1, 3], [0, 3, 12, 4]],<br>                   [[5, 1, 0, 19], [4, 2, -2, 13]]]<br>a &#x3D; np.argmax(three_dim_array, axis &#x3D; 0)<br>print(a)<br>b &#x3D; np.argmax(three_dim_array, axis &#x3D; 1)<br>print(b)<br>c &#x3D; np.argmax(test, axis &#x3D; 2)<br>print(c)<br>[[2 1 0 2]<br> [2 1 1 2]]</p>
<p>[[0 0 0 1]<br> [0 0 1 1]<br> [0 1 0 0]]</p>
<p>[[3 3]<br> [1 2]<br> [3 3]] </p>
<p> ps:这什么鬼东西</p>
<p> <strong>isinstance()</strong> 函数：<br> 来判断一个对象是否是一个已知的类型<br> 语法形式为：isinstance(object, classinfo)<br> object – 实例对象<br> classinfo – 数据类型<br> 返回值：相同则返回 True，否则返回 False。<br>isinstance()函数和type()函数很类似。但是type() 不考虑继承关系。isinstance() 考虑继承关系。<br>如果要判断两个类型是否相同推荐使用 isinstance()。<br>这里给出两个函数区别的示例：<br>class A:<br>    pass<br>class B(A):<br>    pass<br>isinstance(A(), A)    # returns True<br>type(A()) &#x3D;&#x3D; A        # returns True<br>isinstance(B(), A)    # returns True<br>type(B()) &#x3D;&#x3D; A        # returns False</p>
<h2 id="with-torch-no-grad"><a href="#with-torch-no-grad" class="headerlink" title="with torch.no_grad():"></a>with torch.no_grad():</h2><p>非常常见。<br>总结with工作原理：<br>（１）紧跟with后面的语句被求值后，返回对象的“–enter–()”方法被调用，这个方法的返回值将被赋值给as后面的变量；<br>（２）当with后面的代码块全部被执行完之后，将调用前面返回对象的“–exit–()”方法。<br>当requires_grad设置为False时,反向传播时就不会自动求导了，因此大大节约了显存或者说内存。<br>with torch.no_grad的作用<br>在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。<br>即使一个tensor（命名为x）的requires_grad &#x3D; True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。</p>
<h2 id="重要"><a href="#重要" class="headerlink" title="重要"></a>重要</h2><p>在用pytorch训练模型时，通常会在遍历epochs的过程中依次用到<strong>optimizer.zero_grad()</strong> ，<strong>loss.backward()</strong> ，<strong>optimizer.step()</strong> 三个函数<br>这三个函数的作用是先将梯度归零（optimizer.zero_grad()），然后反向传播计算得到每个参数的梯度值（loss.backward()），最后通过梯度下降执行一步参数更新（optimizer.step()）<br>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后<br>使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</p>
<p>忽然发现我把sigmoid()和softmax()函数弄混了<br>他们都是激活函数,但公式不一样<br><img src="/image-40.png" alt="Alt text">用于多类分类问题<br><img src="/image-41.png" alt="Alt text">(sigmoid)</p>
<p>激活函数：ReLU,sigmoid,tanh<br>其中ReLU用的更多(因为计算简单)</p>
<p>通常，我们选择2的若干次幂作为层的宽度</p>
<p><strong>torch.nn.Parameter</strong>继承torch.Tensor，其作用将一个不可训练的类型为Tensor的参数转化为可训练的类型为parameter的参数，并将这个参数绑定到module里面，成为module中可训练的参数。（没看懂）</p>
<p><strong>torch.randn()</strong> 返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。<br>torch.randn(*size, *, out&#x3D;None, dtype&#x3D;None, layout&#x3D;torch.strided, device&#x3D;None, requires_grad&#x3D;False) → Tensor<br>out(Tensor, optional) –输出张量<br>dtype(torch.dtype, optional) –返回张量所需的数据类型。默认:如果没有，使用全局默认值<br>layout(torch.layout, optional) –返回张量的期望布局。默认值:torch.strided<br>device(torch.device, optional) –返回张量的所需 device。默认:如果没有，则使用当前设备作为默认张量类型.(CPU或CUDA)<br>requires_grad(bool, optional) –autograd是否应该记录对返回张量的操作(说明当前量是否需要在计算中保留对应的梯度信息)。默认值:False。</p>
<p>用于对抗过拟合的技术称为正则化（regularization）。<br>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制</p>
<p>通常将我们的数据分成三份， 训练、测试、验证数据集（validation dataset）， 也叫验证集（validation set）。<br><strong>验证集</strong>（validation set）：依靠它来进行模型的选择</p>
<p><strong>K折交叉验证</strong><br>当训练数据稀缺时，将原始训练数据分成K个不重叠的子集。 然后执行K次模型训练和验证，每次在K-1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对K次实验的结果取平均来估计训练和验证误差</p>
<p><strong>power(x, y)</strong> 函数，计算 x 的 y 次方。</p>
<p><strong>np.arange()</strong> 函数<br>语法：<br>numpy.arange(start, stop, step, dtype)<br>start	起始值，默认为0(可不写)<br>stop	终止值（不包含）<br>step	步长，默认为1(可不写)<br>dtype	返回ndarray的数据类型，如果没有提供，则会使用输入数据的类型。</p>
<p><strong>numpy.arange(n).reshape(a, b)</strong>   依次生成n个自然数，并且以a行b列的数组形式显示</p>
<p><strong>numel</strong>函数<br>获取tensor中一共包含多少个元素</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/08/2023-12-8/" data-id="cltl8pnbv00012ot03zz4ffq5" data-title="2023-12-8" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2023-12-6" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/06/2023-12-6/" class="article-date">
  <time class="dt-published" datetime="2023-12-06T02:54:08.000Z" itemprop="datePublished">2023-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/06/2023-12-6/">2023-12-6</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="epoch与batch-size"><a href="#epoch与batch-size" class="headerlink" title="epoch与batch size"></a>epoch与batch size</h2><p>epoch &#x3D; 10 指的是把整个数据集丢进神经网络训练10次。</p>
<p>batch size 指的是数据的个数，batch size &#x3D; 10 指的是每次扔进神经网络训练的数据是10个。</p>
<p>iteration同样指的是次数，iteration &#x3D; 10 指的是把整个数据集分成10次扔进神经网络。<br>假如有100个训练数据，epoch &#x3D; 10, batch size &#x3D; 5, iteration &#x3D; ?<br>interation&#x3D;100&#x2F;5&#x3D;20.<br><img src="/image-7.png" alt="Alt text"></p>
<hr>
<p>机器学习：给数据——&gt;定义模型——&gt;训练&#x2F;<br>神图，偷了&#x2F;<br><img src="/image-8.png" alt="Alt text"><br>mlp是ANN中的一种<br>mxnet与pytorth相似<br><a target="_blank" rel="noopener" href="https://v.subnice.top/api/v1/client/subscribe?token=b570429dacabcd3b06b59c513bfdd745">https://v.subnice.top/api/v1/client/subscribe?token=b570429dacabcd3b06b59c513bfdd745</a><br>成功搭了个梯子(≧∇≦)ﾉ</p>
<p>我还是一点点慢慢学吧，代码看不懂</p>
<h2 id="今天先学数据预处理"><a href="#今天先学数据预处理" class="headerlink" title="今天先学数据预处理"></a>今天先学数据预处理</h2><p>​ 数据预处理的主要内容包括：数据清洗、数据集成、数据变换和数据规约<br><strong>csv</strong>:一种用逗号分隔数据的文件<br><strong>os</strong>:是“operating system”的缩写<br><strong>os.makedirs</strong>(name, mode&#x3D;0o777, exist_ok&#x3D;False)<br><em>作用</em>:<br>用来创建多层目录（单层请用os.mkdir）<br><em>参数说明</em>：<br>name：你想创建的目录名<br>mode：要为目录设置的权限数字模式，默认的模式为 0o777 (八进制)<br>exist_ok：是否在目录存在时触发异常。如果exist_ok为False（默认值），则在目标目录已存在的情况下触发FileExistsError异常；如果exist_ok为True，则在目标目录已存在的情况下不会触发FileExistsError异常</p>
<p><strong>os.path.join</strong>(path1[, path2[, …]])	把目录和文件名合成一个路径<br><strong>iloc[]函数</strong>，属于pandas库，全称为index location<br>例子：</p>
<table>
<thead>
<tr>
<th></th>
<th>姓名（列索引10）</th>
<th>班级（列索引1）</th>
<th>分数（列索引2）</th>
</tr>
</thead>
<tbody><tr>
<td>0（行索引0）</td>
<td>小明</td>
<td>302</td>
<td>87</td>
</tr>
<tr>
<td>1（行索引1）</td>
<td>小王</td>
<td>303</td>
<td>95</td>
</tr>
<tr>
<td>2（行索引2）</td>
<td>小方</td>
<td>303</td>
<td>100</td>
</tr>
</tbody></table>
<p>1.iloc[a,b]:取行索引为a列索引为b的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas</span><br><span class="line">df = pandas.read_csv(&#x27;a.csv&#x27;)</span><br><span class="line">print(df.iloc[1,2])</span><br><span class="line">Out：95</span><br></pre></td></tr></table></figure>
<p>iloc[a:b,c]:取行索引从a到b-1，列索引为c的数据。注意：在iloc中a:b是左到右不到的</p>
<h2 id="get-dummies"><a href="#get-dummies" class="headerlink" title="get_dummies"></a>get_dummies</h2><p>是利用pandas实现one hot encode的方式。<br>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">df = pd.DataFrame([  </span><br><span class="line">            [&#x27;green&#x27; , &#x27;A&#x27;],   </span><br><span class="line">            [&#x27;red&#x27;   , &#x27;B&#x27;],   </span><br><span class="line">            [&#x27;blue&#x27;  , &#x27;A&#x27;]])  </span><br><span class="line"></span><br><span class="line">df.columns = [&#x27;color&#x27;,  &#x27;class&#x27;] </span><br><span class="line">pd.get_dummies(df) </span><br></pre></td></tr></table></figure>
<p>get_dummies 前：<br><img src="/image-9.png" alt="Alt text"><br>get_dummies 后：<br><img src="/image-10.png" alt="Alt text"><br>上述执行完以后再打印df 出来的还是get_dummies 前的图，因为你没有写df &#x3D; pd.get_dummies(df)</p>
<p>可以对指定列进行get_dummies<br>pd.get_dummies(df.color)<br><img src="/image-11.png" alt="Alt text"><br>将指定列进行get_dummies 后合并到元数据中<br>df &#x3D; df.join(pd.get_dummies(df.color))<br><img src="/image-12.png" alt="Alt text">\</p>
<h2 id="范数："><a href="#范数：" class="headerlink" title="范数："></a>范数：</h2><p>  欧几里得距离是一个L2范数：假设n维向量x中的元素是x1,…,xn，其L2范数是向量元素平方和的平方根<br>  $||x||<em>2&#x3D;\sqrt{\sum</em>{i&#x3D;1}^nx_i^2}$<br>  其中，在L2范数中常常省略下标2，也就是说∥x∥等同于∥x∥2。在代码中，我们可以按如下方式计算向量的L2范数。<br>u &#x3D; np.array([3,-4])<br> np.linalg.norm(u)</p>
<p>深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和：<br>$||x||<em>1&#x3D;\sum</em>{i&#x3D;1}^n|x_i|$<br>与L2范数相比，L1范数受异常值的影响较小。为了计算L1范数，我们将绝对值函数和按元素求和组合起来。<br>np.abs(u).sum()<br> array(7.)<br> %.5f：表示按浮点数输出，小数点后面取5位<br> <strong>SVG</strong> 意为可缩放矢量图形（Scalable Vector Graphics）。<br> multinomial distribution:多项式分布<br> binomial:二项式</p>
<p> 为了知道模块中可以调用哪些函数和类，可以调用dir函数。<br>  print(dir(np.random))<br> 有关如何使用给定函数或类的更具体说明，可以调用help函数<br> help(np.ones)</p>
<h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p> ˆ<br> y &#x3D;Xw+b<br> X的每一行是一个样本，每一列是一种特征。权重放到向量w∈Rd中\</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p> 回归问题中最常用的损失函数是平方误差函数。<br> <img src="/image-13.png" alt="Alt text"><br> 话说这玩意好像在数值分析里学过,不过没有1\2<br> 梯度下降（gradientdescent）的方法，这种方法几乎可以优化所有深度学习模型。<br>它通过不断地在损失函数递减的方向上更新参数来降低误差。\</p>
<h2 id="numpy-random-normal"><a href="#numpy-random-normal" class="headerlink" title="numpy.random.normal"></a>numpy.random.normal</h2><p>用例:<br>numpy.random.normal(loc&#x3D;0.0, scale&#x3D;1.0, size&#x3D;None)<br>功能:<br>从正态（高斯）分布中抽取随机样本。<br>loc：标准差<br>scale:方差<br>size:输出值的维度</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/12/06/2023-12-6/" data-id="cltl8pnbx00032ot0drnh79lx" data-title="2023-12-6" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-machine-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/11/07/machine-learning/" class="article-date">
  <time class="dt-published" datetime="2023-11-07T13:14:29.000Z" itemprop="datePublished">2023-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/11/07/machine-learning/">machine learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Sklearn.metrics<br>示例：from sklearn.metrics import mean_squared_error<br>mean_absolute_error 即MAE 平均绝对误差<br>我好像明白为啥推荐用typora了，这儿放不了公式<br>MAE&#x3D;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2023/11/07/machine-learning/" data-id="cltl8pnc000072ot08vwjhprs" data-title="machine learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
          </li>
        
          <li>
            <a href="/2024/07/03/metaverse/">metaverse</a>
          </li>
        
          <li>
            <a href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a>
          </li>
        
          <li>
            <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
          </li>
        
          <li>
            <a href="/2024/05/24/2024-5-24/">2024-5-24</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>