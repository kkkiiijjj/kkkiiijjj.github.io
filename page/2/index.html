<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Forforevery">
<meta property="og:url" content="https://kkkiiijjj.github.io/page/2/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-RNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/19/RNN/" class="article-date">
  <time class="dt-published" datetime="2024-04-19T02:26:46.000Z" itemprop="datePublished">2024-04-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/19/RNN/">RNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在看enconder代码的时候发现里面竟然有RNN<br>我还是好好整理一下RNN吧</p>
<p>enconder里的RNN层的作用：<br>实例：（复制chatgpt的，嫌长可以不看）</p>
<ol>
<li>Sentence: “The movie was a disappointment, the acting was terrible.”<br> Processing Words Sequentially: The RNN in the encoder processes the sentence word by word:</li>
</ol>
<p>Word 1: “The”</p>
<p>The RNN receives the embedding vector for “The” and combines it with the initial hidden state (often initialized with zeros).<br>Based on this combination, the RNN updates the hidden state h_1. This initial hidden state doesn’t hold much sentiment information yet.<br>Word 2: “movie”</p>
<p>The RNN receives the embedding vector for “movie” and combines it with the updated hidden state h_1.<br>Now, h_1 contains some context about “The,” which might be neutral. The RNN updates the hidden state h_2 to reflect this context along with the meaning of “movie.”<br>Word 3: “was”</p>
<p>The process repeats. The RNN receives the embedding vector for “was” and combines it with the previous hidden state h_2.<br>h_2 carries information about “The movie,” which could be neutral or slightly positive depending on the embedding vectors. The RNN updates the hidden state h_3 to incorporate the meaning of “was” in this context.<br>Word 4: “a” (similar to “The”)</p>
<p>Word 5: “disappointment”</p>
<p>The RNN receives the embedding vector for “disappointment,” a negative word.<br>The previous hidden state h_4 likely held some neutral or slightly positive sentiment. The RNN updates h_5 to reflect the strong negative connotation of “disappointment,” shifting the overall sentiment towards negative.<br>Words 6-7 follow similarly, likely reinforcing the negative sentiment.</p>
<ol start="3">
<li>Final Hidden State: After processing all words, the RNN in the encoder reaches the final hidden state, let’s call it h_final. This state encapsulates the sentiment of the entire sentence, considering the sequence of words and their relationships.</li>
<li>Using the Hidden State: The h_final can then be used for various tasks:</li>
</ol>
<p>Sentiment Classification: This hidden state can be fed into a classifier (e.g., a simple neural network) to predict the overall sentiment of the sentence (likely negative in this case).<br>Machine Translation: If this is part of a machine translation system, the h_final might be used to encode the meaning of the sentence and generate a corresponding translation in another language.</p>
<p>简而言之，RNN每步读入当前的Word并将其与之前的隐状态结合、更新隐状态，隐状态反映当前输入的所有词之间的relationship<br>在读完整个句子后，我们会得到最后的隐状态，最后的隐状态包含了整句话的所有词之间的relationship<br>最后的隐状态可用于情感分类、机器翻译</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/19/RNN/" data-id="clv61usel0000qgt0gk46hjjd" data-title="RNN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Kaggle-LLM-Science-Exam" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/Kaggle-LLM-Science-Exam/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T06:43:35.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/Kaggle-LLM-Science-Exam/">Kaggle - LLM Science Exam</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>目的：this competition challenges participants to answer difficult science-based questions written by a Large Language Model.<br>任务：预测前三个最可能是正确答案的选项<br>评价标准：Mean Average Precision @ 3 (MAP@3):<br><img src="/image-107.png" alt="alt text"></p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>train集:包括一个问题、五个选项及正确答案<br>test集:是train集的copy,只是没了answer列(啊？这不妥妥过拟合了吗？)</p>
<h2 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h2><h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>选用Embedding模型：gte-base<br>    还有很多其他Embeding方法，如bge-large-en-v1.5\bge-base-en-v1.5\ember-v1\gte-large\e5-large\bge-small<br>不一定哪个表现最好，视具体情况而定<br>可以拿小样本跑一跑看哪个表现好</p>
<p><img src="/image-108.png" alt="alt text"><br>这里有个问题是若使用检索2的话会使Prompt占的比重很小</p>
<p>一些tricks:若GPU性能没那么好，就调小batch_size(1或2)<br>若改成1还跑不了就调小max_input</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/Kaggle-LLM-Science-Exam/" data-id="clv3g5d5p0000bgt0a3la0wud" data-title="Kaggle - LLM Science Exam" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle—HMS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/kaggle%E2%80%94HMS/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T00:57:30.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/kaggle%E2%80%94HMS/">kaggle—HMS</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>兜兜转转犹犹豫豫最后还是在淘宝上买了kaggle竞赛的课<br>花了我3600大洋呜呜呜呜，不过可以保证我拿银牌，我心里就有底了</p>
<p>那么废话不多说，第一课讲的是HMS</p>
<p>打比赛的流程：<br><img src="/image-100.png" alt="alt text"></p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>比赛目的：检测分类癫痫发作和其他类型的有害大脑活动。<br>比赛任务：开发一个model,根据从医院重症患者记录的脑电图(EEG)信号进行训练。<br>评价指标：KL散度(不常用)</p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>data:<br>这数据有啥特征啊。。。</p>
<p><img src="/image-103.png" alt="alt text"><br><img src="/image-101.png" alt="alt text"><br>train数据集包括50秒的EEG(相应匹配于频谱图)<br>[seizure&#x2F;lpd&#x2F;gpd&#x2F;lrda&#x2F;grda&#x2F;other]_vote：<br>人工打分的各类疾病投票<br>一共有6个label，分别是seizure(癫痫)、GRDA、Other、GPD、LRDA、LPD<br>(查不到都是啥病)</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>用librosa包进行特征提取</p>
<h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p><img src="/image-102.png" alt="alt text"><br><img src="/image-104.png" alt="alt text"><br>方案一直接用给的数据<br><img src="/image-105.png" alt="alt text"><br>方案二用提取特征后的<br><img src="/image-106.png" alt="alt text"><br>方案三用给的+提取特征后的(效果最好)</p>
<p>提分要点：使用vote&gt;10的样本二次训练<br>(即第一次用全部的train集训练，第二次用vote高的进行训练，因为vote高代表准确率更高，但即便如此，vote少的依然可以提供一些信息(钱少也是钱啊))</p>
<p>大致的思路我懂了，但是没怎么看代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/kaggle%E2%80%94HMS/" data-id="clv33sa2q0000ekt0akf20t8c" data-title="kaggle—HMS" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-prompt" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/13/prompt/" class="article-date">
  <time class="dt-published" datetime="2024-04-13T01:49:55.000Z" itemprop="datePublished">2024-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/13/prompt/">prompt</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近在做大创，是关于数字人的，这个项目我很感兴趣，感觉未来可以应用的范围很广，前景也很好。<br>等以后元宇宙火起来了感觉这个会很赚钱</p>
<p>好了回到正题<br><img src="/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240413094011.jpg" alt="alt text"><br>这是我们模型的框架</p>
<h2 id="关于prompt"><a href="#关于prompt" class="headerlink" title="关于prompt:"></a>关于prompt:</h2><p>即提示词(咒语)，是LLM(large language model)的重要起点</p>
<h2 id="RAG检索"><a href="#RAG检索" class="headerlink" title="RAG检索"></a>RAG检索</h2><p>(Retrieval Augmented Generation)<br>结合prompt和其他相关资料，使得LLM的能力更上一层楼<br>RAG可用于回答问题、总结、fact checking、对话系统</p>
<h2 id="多模态融合"><a href="#多模态融合" class="headerlink" title="多模态融合"></a>多模态融合</h2><p>深度学习多模态融合指机器从文本、图像、语音、视频等多个领域获取信息</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/13/prompt/" data-id="cluxfwb0g00009ot0h4bpfvad" data-title="prompt" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LSTM" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/08/LSTM/" class="article-date">
  <time class="dt-published" datetime="2024-04-08T08:44:35.000Z" itemprop="datePublished">2024-04-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/08/LSTM/">LSTM</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>是RNN的升级版，用于机器翻译，语音辨识，时间序列预测，文本生成</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>使用门控单元，使得能够长期记忆<br>缓解梯度消失和梯度爆炸</p>
<h2 id="门控单元"><a href="#门控单元" class="headerlink" title="门控单元"></a>门控单元</h2><p>使用门控单元，包括：</p>
<ol>
<li>输入门   数据读入</li>
<li>遗忘门   重置单元内容</li>
<li>输出门   数据输出<br><img src="/image-93.png" alt="alt text"><br>图为这仨门的公式，红色框起的是输入，桔色是参数(b是偏置参数)，黄色是隐状态</li>
</ol>
<h2 id="候选记忆元"><a href="#候选记忆元" class="headerlink" title="候选记忆元"></a>候选记忆元</h2><p>与上面仨门类似，不同点是使用tanh作为激活函数<br><img src="/image-94.png" alt="alt text"><br><img src="/image-95.png" alt="alt text"></p>
<h2 id="记忆元"><a href="#记忆元" class="headerlink" title="记忆元"></a>记忆元</h2><p>公式:<br><img src="/image-96.png" alt="alt text"><br>输入门$I_t$控制采用多少来自$C_t$的新数据， 而遗忘门$F_t$控制保留多少过去的 记忆元$C_{t-1}$的内容。<br><img src="/image-97.png" alt="alt text"></p>
<h2 id="关于隐状态在LSTM中的使用："><a href="#关于隐状态在LSTM中的使用：" class="headerlink" title="关于隐状态在LSTM中的使用："></a>关于隐状态在LSTM中的使用：</h2><p>隐状态将输入的词转换为数字表示，<br>LSTM基于当前的  输入  &amp;  更新的隐状态  来预测下一个单词，如图：<br><img src="/image-99.png" alt="alt text"><br>公式：<img src="/image-98.png" alt="alt text"><br>这咋上来就遗忘呢，为啥不先输入？？</p>
<p>只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p>
<p>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/08/LSTM/" data-id="cluqpickq00004gt07w63bw7n" data-title="LSTM" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-房价预测代码解读" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" class="article-date">
  <time class="dt-published" datetime="2024-03-25T06:09:16.000Z" itemprop="datePublished">2024-03-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">房价预测代码解读</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这里解析的是李沐深度学习4.10的代码</p>
<h2 id="导入数据："><a href="#导入数据：" class="headerlink" title="导入数据："></a>导入数据：</h2><pre><code>用pd.read_csv(&quot;&quot;)导入本地数据,()内填写地址
</code></pre>
<p>没什么问题</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><pre><code>将数据的第一列即序号列去掉并将数据集与测试集通过concat()函数连接到一起

这么做的原因是进行standardize和normalize时需用到全部数据,如果分别进行的话会使训练集和测试集的mean和standard deviation不一样，进而影响模型表现
但这么做可能导致leakage(泄露)：比如用训练集的mean去填充测试集的空缺值

是否应该concatenate取决于选择了什么方式填充空缺值
适用concatenate的情况：用所有数据的mean\median\mode(众数)填充
不适用：KNN(K折交叉验证)、插值法、决策树
当不确定是否会造成leakage时，可以总选择分别进行
</code></pre>
<p>这里用的时0填充，所以concat与否无所谓</p>
<p>一行代码需要解释的竟然这么多，读的我好累（</p>
<p>‘’’<br>all_features &#x3D; pd.get_dummies(all_features, dummy_na&#x3D;True)<br>‘’’<br>此行代码将categorical variables转换成dummy variables，即将文字类型转换成数字类型</p>
<p>而后把数据转换为张量并将之前连在一起的数据集和测试集分开</p>
<p>然后定义net,这里用的是线性模型<br>感觉nn.Linear()在这里多余，因为只有一层，但是chatgpt告诉我即使是一层用这个也是有好处的<br>让我不再需要定义forward方法</p>
<p>然后他重定义了loss函数<br>但我不明白为什么不用原来的MSELoss</p>
<h2 id="定义train函数"><a href="#定义train函数" class="headerlink" title="定义train函数"></a>定义train函数</h2><p>这个train函数返回的是两个列表，分别是train和test的loss值<br>这里用了d2l包里的迭代器 分批加载数据<br>然后选择Adam作为优化算法<br>接着是常规操作</p>
<h2 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h2><p>定义函数get_k_fold_data(k, i, X, y)<br>参数：<br>    k:将数据分成k折<br>    i:验证集<br>    X:包含features的张量</p>
<p>返回的值：<br>    X_train:train集 的 features<br>    y_train:train集 的labels<br>    X_valid:validation集 的 features<br>    y_valid:validation集 的 labels</p>
<p>assert k &gt; 1 下断言，保证k&gt;1<br>然后计算fold_size的大小，即每一批有多少数据点<br>循环内将数据分成训练集和验证集</p>
<p>下一个函数def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,<br>           batch_size)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" data-id="clu6jsnhc0000bst0fcgi18nt" data-title="房价预测代码解读" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-马尔可夫" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" class="article-date">
  <time class="dt-published" datetime="2024-03-24T07:15:03.000Z" itemprop="datePublished">2024-03-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/">马尔可夫</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>关于隐状态：<br>从过去的输入中提取重要特征并用于预测当前或下一个output<br>RNN会一步步更新隐状态，这个更新过程会将 目前的输入 与 去除过去不相关信息后的信息 结合在一起<br>这里举一个经典的例子：有很多盒子。每个盒子里装着不同的球，每次随机从一个盒子里摸出一个球<br>在这个过程中，盒子出现的顺序是隐含状态序列，球出现的顺序是观测序列，隐含状态之间的转换过程是一个马尔可夫过程</p>
<p>GRU(门控单元)<br>包含重置门（reset gate）和更新门（update gate）<br>具体运算：<br><img src="/image-87.png" alt="alt text"><br><img src="/image-88.png" alt="alt text"><br><img src="/image-89.png" alt="alt text"></p>
<p>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</p>
<p>重置门有助于捕获序列中的短期依赖关系。</p>
<p>更新门有助于捕获序列中的长期依赖关系。</p>
<p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p>
<p>自回归模型（autoregressive models）<br>Mechanics：使用线性回归根据过去的数据预测将来，比如天气预报通过分析前几天的温度来预测接下来几天的温度</p>
<p>自回归和马尔可夫的区别：自回归依赖过去的数据，而马尔可夫不，后者仅依赖current state<br>当数据之间存在线性关系时用自回归，短期预测用马尔可夫</p>
<p>HMM(隐马尔可夫)与马尔可夫的区别<br><img src="/image-90.png" alt="alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" data-id="clu56pfe40000sot0322g9ubw" data-title="马尔可夫" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-链表力扣刷题" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/22/%E9%93%BE%E8%A1%A8%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/" class="article-date">
  <time class="dt-published" datetime="2024-03-22T09:31:32.000Z" itemprop="datePublished">2024-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/22/%E9%93%BE%E8%A1%A8%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/">链表力扣刷题</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>开始刷题啦！<br>从链表开始刷的，顺序参考的是代码随想录<br>这里记录下每道题的大致思路：</p>
<ol>
<li>移除等于所给值的链表元素(203)<br> (1)递归<br> 首先考虑返回值，想返回的是去掉特定节点的链表，而考虑到递归是一步步返回的，所以返回的数据形式应该是节点。<br> 然后返回的上一行代码应该是调用函数本身<br> (2)迭代+设置虚拟头节点</li>
<li>设计单链表与双链表(707)<br>实现<br>addAtIndex(index,val)：在链表中的第 index 个节点之前添加值为 val  的节点。如果 index 等于链表的长度，则该节点将附加到链表的末尾。如果 index 大于链表长度，则不会插入节点。如果index小于0，则在头部插入节点。<br>新建节点保存要插入的值<br>（这个有待完善）</li>
<li>反转链表（206）<br>(1)递归<br>(2)迭代(双指针法（虽然我觉得其实是三指针）)<br> 一个在头，一个在尾(null)<br> 然后四步走：<br> a.新建一个节点储存头的下一个节点<br> 然后就是挪来挪去了:<br>             b.头的下一个指向尾<br>             c.更新尾节点<br>             d.更新头节点</li>
<li>两两交换链表中的节点(24)<br>(1)递归<br>(2)迭代+dummyhead<br>(这个比较简单)</li>
<li>删除链表的倒数第N个节点(19)<br>(1)化为正序删除<br>(2)栈<br>(3)双指针</li>
<li>链表相交(面试题02.07)<br>(1)哈希<br>先把A链表的所有元素放进哈希表里，再遍历B链表看什么时候重合。<br>(2)双指针<br>给两个链表的头节点，返回两链表的相交节点，若不相交，返回NULL<br>我觉得这是一道数学题<br>创建两个节点(如p,q)，分别指向两个链表的头，哪个把当前所在的链走完了就去走另外一条链，最后二者相遇的那个节点即为所求<br>就像互相暗恋的两个人，从陌生到相知，然后互相试探（你往前走一步我往前走一步），结果俩人总是隔着那么几步，然后其中一个忽然发现了对方喜欢自己的证据，另一个稍后也发现了，证据越来越多最后俩人都发现对方喜欢自己，然后他们在一起了，皆大欢喜。<br>（编得我都感动了）<br>好了回到正题：这种题肯定会用到循环，然后要想循环的终止条件是什么，我们希望最后俩指针重合</li>
<li>环形链表<br>(1)哈希<br>循环结束条件为head为空，每次循环检查当前节点是否在哈希表中，不在则加且将head向前移动一步，在则返回头节点<br>(2)快慢指针<br>依然是一道数学题，而且是追及问题<br>快的一次移两步，慢的一步，假如有环，他们终将相遇，并且在相遇的这个点，新建一个指针指向头，同时释放两球（不是），让他和慢指针同步移动，最后二者将在入环点相遇<br>数学原理如下：<br><img src="/image-91.png" alt="alt text"></li>
</ol>
<p>所以哈希表很适合判定重复对吧？<br>然后关于dummyhead的使用：<br>dummyhead的好处：<br>(1)处理边缘值更容易<br>(2)无需检查即可access头节点<br>(3)便于插入与删除</p>
<p>以及递归方法的使用：<br>(1)if判断句(比如判空)<br>(2)自调语句<br>(3)其他操作(可选)<br>(4)返回<br>我发现这个递归消耗内存好像普遍比较多啊</p>
<p>链表题中用到递归的几个：<br><img src="/image-109.png" alt="alt text"><br><img src="/image-110.png" alt="alt text"><br><img src="/image-111.png" alt="alt text"><br>可以感受一下递归的格式</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/22/%E9%93%BE%E8%A1%A8%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/" data-id="clu2gp6kk000048t01xsq43zq" data-title="链表力扣刷题" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-梯度消失与梯度爆炸" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/" class="article-date">
  <time class="dt-published" datetime="2024-03-22T07:57:58.000Z" itemprop="datePublished">2024-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/">梯度消失与梯度爆炸</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>产生原因：(1)层数太深，输入的数多次乘以小于1的权重得到一个趋于零的值导致梯度消失，或是乘一个大于1的权重得到趋于无穷的值导致梯度爆炸<br>        (2)激活函数选择不当<br>影响：无法进一步更新权重值（因为更新权重是要将梯度与零对比的，趋于零就不再更新了，而大太多则会一直更新，非常缓慢）<br>解决方法：</p>
<ul>
<li>预训练加微调</li>
<li>梯度剪切、权重正则 （针对梯度爆炸）</li>
<li>使用不同的激活函数 （如relu、leakrelu、elu等激活函数）</li>
<li>使用batchnorm</li>
<li>使用残差结构</li>
<li>使用LSTM网络</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/" data-id="clu2dd20s0000d0t00fa63vj1" data-title="梯度消失与梯度爆炸" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-time-machine-code解读" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/13/time-machine-code%E8%A7%A3%E8%AF%BB/" class="article-date">
  <time class="dt-published" datetime="2024-03-13T02:46:02.000Z" itemprop="datePublished">2024-03-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/13/time-machine-code%E8%A7%A3%E8%AF%BB/">time machine code解读</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="/image-83.png" alt="alt text"><br>一个例子：（我觉得果然还是例子更利于帮助理解）<br><img src="/image-84.png" alt="alt text"><br>ps:z这个例子是chatgpt给我的，太好用了呜呜呜</p>
<p><img src="/image-85.png" alt="alt text"><br>if部分判断输入是否符合标准，前者判断是否为空，后者判断是否是一个列表而非word<br>使用示例：<br><img src="/image-86.png" alt="alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/13/time-machine-code%E8%A7%A3%E8%AF%BB/" data-id="cltpismf20000k4t01qx05yn5" data-title="time machine code解读" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/13/Cursor%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">Cursor使用教程</a>
          </li>
        
          <li>
            <a href="/2024/12/13/llm%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90-%E6%A1%8C%E5%AE%A0/">llm项目解析-桌宠</a>
          </li>
        
          <li>
            <a href="/2024/12/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90/">大模型小项目解析</a>
          </li>
        
          <li>
            <a href="/2024/08/15/llm%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/">llm训练方法概述</a>
          </li>
        
          <li>
            <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>