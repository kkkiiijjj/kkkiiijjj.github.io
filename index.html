<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Forforevery">
<meta property="og:url" content="https://kkkiiijjj.github.io/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-llm应用开发" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/" class="article-date">
  <time class="dt-published" datetime="2024-08-01T10:37:08.000Z" itemprop="datePublished">2024-08-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/" data-id="clzb561dr0000qkt051l00u13" data-title="llm应用开发" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-metaverse" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/07/03/metaverse/" class="article-date">
  <time class="dt-published" datetime="2024-07-03T02:40:03.000Z" itemprop="datePublished">2024-07-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/07/03/metaverse/">metaverse</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在规划我的未来路线时忽然想起之前很火的一个名词————metaverse<br>然后想想估计很有前景而且我也很感兴趣</p>
<p>于是去搜了几篇论文，也不知道我这种查找资料的方式对不对<br>论文链接：<br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=ljHcwhvF5s0D3XXrGK85BedHtACuRUDSSU5Pzjly8BimR92nVbEF04PeMa89ciBdiAB3f/dEeS5LX/yF0UnUuvO8/ACxlT4pqLwuUtTvHHVh1HKkl99dbeirtmqj1HWo+g5ajRIakcVqZuR5kQtP+eGP/L71SGQKiGkdQ6IJQtY=&DBCODE=CJFQ&FileName=NEWS202110008&TABLEName=cjfdlast2021&nonce=FE8BE552A6D94C34BF2260AEB757A31A&TIMESTAMP=1719977974607&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=ljHcwhvF5s0D3XXrGK85BedHtACuRUDSSU5Pzjly8BimR92nVbEF04PeMa89ciBdiAB3f%2FdEeS5LX%2FyF0UnUuvO8%2FACxlT4pqLwuUtTvHHVh1HKkl99dbeirtmqj1HWo%2Bg5ajRIakcVqZuR5kQtP%2BeGP%2FL71SGQKiGkdQ6IJQtY%3D&amp;DBCODE=CJFQ&amp;FileName=NEWS202110008&amp;TABLEName=cjfdlast2021&amp;nonce=FE8BE552A6D94C34BF2260AEB757A31A&amp;TIMESTAMP=1719977974607&amp;uid=</a><br>(这篇是被引最多的)<br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=nd8tH6Gp8qusAUBJeVilhVKqvQpVV84H437QH2hJqpnLJVit7CkDvQbXpZfDbocpF0SzafPlOqDs4rZBqOoqQMhH36JjrKYhgA9Vsx8UGVvANv/AkazvvwbcxmIczDk8icEYpamtJvYLnBYggJ/cJkbK3agFnW+BPlsMIU6YJ8g=&DBCODE=CJFQ&FileName=XJSF202203010&TABLEName=cjfdlast2022&nonce=3CA8110B039B40C6B82153C4D3B59369&TIMESTAMP=1719803796798&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=nd8tH6Gp8qusAUBJeVilhVKqvQpVV84H437QH2hJqpnLJVit7CkDvQbXpZfDbocpF0SzafPlOqDs4rZBqOoqQMhH36JjrKYhgA9Vsx8UGVvANv%2FAkazvvwbcxmIczDk8icEYpamtJvYLnBYggJ%2FcJkbK3agFnW%2BBPlsMIU6YJ8g%3D&amp;DBCODE=CJFQ&amp;FileName=XJSF202203010&amp;TABLEName=cjfdlast2022&amp;nonce=3CA8110B039B40C6B82153C4D3B59369&amp;TIMESTAMP=1719803796798&amp;uid=</a><br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=oLlOB/q0RPqvHaS5NBtXuIIwWfHV2gPO7AiDUhskGTfocKKzHaVlvmPHQhUuWrLs7GpX4mt5oV0bmscV2/W0Dqu1N4n79tghMBrFGWm8KUr7xH1qtw3cRig163LJR4HSER8FYSabbMEsoKuO9OijzLTdDJ7eOssK4MymxjSM1QE=&DBCODE=CJFQ&FileName=XNZY202204002&TABLEName=cjfdlast2022&nonce=D3C7584445A74505835C153C544F2E1F&TIMESTAMP=1719804554876&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=oLlOB%2Fq0RPqvHaS5NBtXuIIwWfHV2gPO7AiDUhskGTfocKKzHaVlvmPHQhUuWrLs7GpX4mt5oV0bmscV2%2FW0Dqu1N4n79tghMBrFGWm8KUr7xH1qtw3cRig163LJR4HSER8FYSabbMEsoKuO9OijzLTdDJ7eOssK4MymxjSM1QE%3D&amp;DBCODE=CJFQ&amp;FileName=XNZY202204002&amp;TABLEName=cjfdlast2022&amp;nonce=D3C7584445A74505835C153C544F2E1F&amp;TIMESTAMP=1719804554876&amp;uid=</a></p>
<p>来源：小说《雪崩》<br>已有相关部署的公司：微软、Facebook、Roblox(元宇宙第一股)<br>国内的有字节、腾讯、网易、莉莉丝、米哈游、中青宝等游戏公司<br>中心技术：区块链、交互技术、电子游戏技术、人工智能技术、智能网络技术、物联网技术<br>区块链：是实现去中心化的分布式社会中人与人之间信任、协同的技术基础<br>包含要素：Roblox的首席执行官大卫.巴斯祖基认为至少包括身份、朋友、沉浸感、低延迟、多元化、随地、经济系统和文明要素<br>突破点：实现人的嗅觉、味觉及触觉等感官效应的线上化<br>终端要足够便携、易佩戴（“瘦终端、胖云端”）<br>VR、AR、MR等扩展现实<br><img src="/image-116.png" alt="alt text"><br><img src="/image-117.png" alt="alt text"><br><img src="/image-118.png" alt="alt text"><br>基本特征：去中心化<br>四大核心属性：<br>1.与现实世界的同步性与高拟真度。元宇宙虚拟空间与现实社会保持高度同步和互通，交互效果逼近真实。具有同步性和高拟真度的虚拟世界是元宇宙构成的基础条件，它意味着现实社会中发生的一切事件将同步于虚拟世界，同时用户在虚拟的元宇宙中进行交互时能得到近乎真实的反馈信息。<br>2.开源开放与创新创造。开源开放是指技术开源和平台开源，元宇宙通过制定“标准”和“协议”将代码进行不同程度的封装<br>和模块化，不同需求的用户都可以在元宇宙进行自主创新和创造，构建原创的虚拟世界，不断拓展元宇宙边界。<br>3.永续发展。元宇宙平台的建设和发展不会“暂停 或“结束”，而是以开源开放的方式运行并无限期地持续发展.<br>4.拥有闭环运行的经济系统。在元字宙申，用户的生产和工作活动的价值将以平台统一的货币形式被确认和确权，用户可以使用这一货币在元宇宙平台内进行消费，也可以通过一定比例“兑换”成现实生活中的法定货币。毫无疑问，经济系统的闭环运行是驱动和保障元宇宙不断变化和发展的动力引擎。</p>
<p>建议朗读全文：<br>《2022胡润中国元宇宙潜力企业榜》<br><a target="_blank" rel="noopener" href="http://www.pjtime.com/2022/6/322149147125.shtml#:~:text=%E8%83%A1%E6%B6%A6%E7%A0%94%E7%A9%B6%E9%99%A22022%E5%B9%B46%E6%9C%8815%E6%97%A5%E4%BA%8E%E5%B9%BF%E5%B7%9E%E5%8D%97%E6%B2%99%E5%8F%91%E5%B8%83%E3%80%8A2022%E8%83%A1%E6%B6%A6%E4%B8%AD%E5%9B%BD%E5%85%83%E5%AE%87%E5%AE%99%E6%BD%9C%E5%8A%9B%E4%BC%81%E4%B8%9A%E6%A6%9C%E3%80%8B%EF%BC%88Hurun%20China%20Metaverse%20Companies,with%20the%20Greatest%20Potential%202022%EF%BC%89%EF%BC%8C%E5%88%97%E5%87%BA%E4%BA%86%E5%85%83%E5%AE%87%E5%AE%99%E9%A2%86%E5%9F%9F%E6%9C%80%E5%85%B7%E5%8F%91%E5%B1%95%E6%BD%9C%E5%8A%9B%E7%9A%84%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A200%E5%BC%BA%EF%BC%8C%E5%88%86%E4%B8%BA%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9C%80%E5%85%B7%E6%BD%9C%E5%8A%9B20%E5%BC%BA%E3%80%8150%E5%BC%BA%E3%80%81100%E5%BC%BA%E5%92%8C200%E5%BC%BA%E3%80%82">http://www.pjtime.com/2022/6/322149147125.shtml#:~:text=%E8%83%A1%E6%B6%A6%E7%A0%94%E7%A9%B6%E9%99%A22022%E5%B9%B46%E6%9C%8815%E6%97%A5%E4%BA%8E%E5%B9%BF%E5%B7%9E%E5%8D%97%E6%B2%99%E5%8F%91%E5%B8%83%E3%80%8A2022%E8%83%A1%E6%B6%A6%E4%B8%AD%E5%9B%BD%E5%85%83%E5%AE%87%E5%AE%99%E6%BD%9C%E5%8A%9B%E4%BC%81%E4%B8%9A%E6%A6%9C%E3%80%8B%EF%BC%88Hurun%20China%20Metaverse%20Companies,with%20the%20Greatest%20Potential%202022%EF%BC%89%EF%BC%8C%E5%88%97%E5%87%BA%E4%BA%86%E5%85%83%E5%AE%87%E5%AE%99%E9%A2%86%E5%9F%9F%E6%9C%80%E5%85%B7%E5%8F%91%E5%B1%95%E6%BD%9C%E5%8A%9B%E7%9A%84%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A200%E5%BC%BA%EF%BC%8C%E5%88%86%E4%B8%BA%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9C%80%E5%85%B7%E6%BD%9C%E5%8A%9B20%E5%BC%BA%E3%80%8150%E5%BC%BA%E3%80%81100%E5%BC%BA%E5%92%8C200%E5%BC%BA%E3%80%82</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/07/03/metaverse/" data-id="cly58cryc0000q0t0h6za5qnu" data-title="metaverse" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-注意力机制" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="article-date">
  <time class="dt-published" datetime="2024-06-24T01:37:03.000Z" itemprop="datePublished">2024-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>废话：我发现看电子版的书有一个弊端，就是容易熊瞎子匹苞米，看了后面的就忘了前面的，每个章节之间的联系不清楚，缺乏整体的一个总览。可能人类已经习惯阅读纸质书了</p>
<h2 id="注意力提示"><a href="#注意力提示" class="headerlink" title="注意力提示"></a>注意力提示</h2><h3 id="人的注意力"><a href="#人的注意力" class="headerlink" title="人的注意力"></a>人的注意力</h3><p>主要包括  自主性的  与  非自主性的  注意力</p>
<h3 id="查询、键和值-query-key-and-value"><a href="#查询、键和值-query-key-and-value" class="headerlink" title="查询、键和值(query,key and value)"></a>查询、键和值(query,key and value)</h3><p><img src="/image-113.png" alt="alt text"><br>个人理解就是key是外在物品的特征，而query是个人的内在特征<br>值是可供选择的事物</p>
<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3><p>Nadaraya-Watson核回归（Nadaraya-Watson kernel regression）:根据输入的位置对输出y_i进行加权<br>注意力汇聚（attention pooling）公式：<br><img src="/image-114.png" alt="alt text"><br>其中x是查询，$(x_i,y_i)$是键值对<br>x和key$x_i$之间的关系建模为注意力权重（attention weight）:α(x,$x_i$)<br>非参数的Nadaraya-Watson核回归具有一致性（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 </p>
<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3><p>在下面的查询x和键$x_i$之间的距离乘以可学习参数w<br><img src="/image-115.png" alt="alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" data-id="clxsb52zg0000ykt04sz46xoe" data-title="注意力机制" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle竞赛——AI-Mathematical" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" class="article-date">
  <time class="dt-published" datetime="2024-06-22T06:41:17.000Z" itemprop="datePublished">2024-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>OK,这是我参加的第一个kaggle比赛<br>还花钱报了班<br>比赛6月28出成绩结果到现在我一个代码都还没敲(汗)<br>比赛传送门：<a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/overview">https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/overview</a></p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>AI Mathematical Olympiad - Progress Prize 1<br>目的：创建算法和model解决困难的数学问题(问题是LaTex格式的)，旨在帮助AI模型提高数学逻辑推理能力<br>任务：给出test集的数学题的答案<br>评价标准：预测值的准确度</p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>train集：包含十个问题及其答案的.csv文件<br>test集：说是给了50个问题，但是我只看到了3个</p>
<h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>好像很多人用的Gemma</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" data-id="clxpr4tyx00009wt0dmo3hmks" data-title="kaggle竞赛——AI Mathematical" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-5-24" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/24/2024-5-24/" class="article-date">
  <time class="dt-published" datetime="2024-05-24T08:27:09.000Z" itemprop="datePublished">2024-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/24/2024-5-24/">2024-5-24</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一些废话：在看kaggle竞赛的课<br>感觉讲的不是很行啊，听半天感觉和没听一样</p>
<p>竞赛：LLM Prompt Recovery | Metric Computation<br>比赛地址直达：<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation">https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation</a><br>目的：由原始文本与Gemma改写的文本段落反推提示词(prompt)<br>评价指标：<br>锐化余弦相似度<img src="/image-112.png" alt="alt text"><br>可以用三个不同的模型来解决</p>
<ol>
<li>Seq2Seq</li>
<li>few-shot<br>zero-shot LLM       没有示例<br>one-shot LLM        给一个示例<br>few-shot LLM        给一些示例</li>
<li>Phi 2</li>
</ol>
<p>又是一段废话：<br>然后好像没有有用的话了，code给的还特别小，是想让我瞎吗()<br>好在可以管客服索要代码，看网课不如看代码<br>代码老长老长，给的是.ipynb格式的文件，据说要用juypter打开，不过我用vscode也打开了。<br>觉得自己有必要抽空学一下juypter</p>
<p>代码在这里放一下吧还是</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>%%writefile trian_embedding_generate.py<br>import pandas as pd<br>import gc<br>import numpy as np<br>import pandas as pd<br>import time<br>from tqdm import tqdm<br>import numpy as np<br>from sentence_transformers import SentenceTransformer<br>import pickle</p>
<p>df &#x3D; pd.read_parquet(f”.&#x2F;train_clean.parquet”, columns&#x3D;[‘rewrite_prompt’])</p>
<h4 id="这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下"><a href="#这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下" class="headerlink" title="这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)"></a>这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)</h4><p>valid &#x3D; pd.read_csv(‘.&#x2F;validation826.csv’, usecols&#x3D;[‘rewrite_prompt’])</p>
<h4 id="两个文件均只导入特定列"><a href="#两个文件均只导入特定列" class="headerlink" title="两个文件均只导入特定列"></a>两个文件均只导入特定列</h4><p>model &#x3D;  SentenceTransformer(‘sentence-transformers&#x2F;sentence-t5-base’)#</p>
<h4 id="选择SentenceTransformer-model进行embedding"><a href="#选择SentenceTransformer-model进行embedding" class="headerlink" title="选择SentenceTransformer model进行embedding"></a>选择SentenceTransformer model进行embedding</h4><p>model.max_seq_length &#x3D; 512</p>
<h4 id="最长序列长度设为512"><a href="#最长序列长度设为512" class="headerlink" title="最长序列长度设为512"></a>最长序列长度设为512</h4><p>encoded_data &#x3D; model.encode(list(df[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>encoded_data &#x3D; encoded_data.detach().cpu().numpy()<br>encoded_data &#x3D; np.asarray(encoded_data.astype(‘float32’))</p>
<p>np.save(‘train_clean_emb_sentence-t5-base.npy’, encoded_data)</p>
<p>valid_emb &#x3D; model.encode(list(valid[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>valid_emb &#x3D; valid_emb.detach().cpu().numpy()<br>valid_emb &#x3D; np.asarray(valid_emb.astype(‘float32’))</p>
<p>np.save(‘valid826_emb_sentence-t5-base.npy’, valid_emb)</p>
<h4 id="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里"><a href="#这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里" class="headerlink" title="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里"></a>这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里</h4><h1 id="seq2seq训练"><a href="#seq2seq训练" class="headerlink" title="seq2seq训练"></a>seq2seq训练</h1><h4 id="the-config-class-provides-a-structured-way-to"><a href="#the-config-class-provides-a-structured-way-to" class="headerlink" title="the config class provides a structured way to"></a>the config class provides a structured way to</h4><h4 id="define-and-manage-the-various-hyperparameters-and-configuration-settings"><a href="#define-and-manage-the-various-hyperparameters-and-configuration-settings" class="headerlink" title="define and manage the various hyperparameters and configuration settings"></a>define and manage the various hyperparameters and configuration settings</h4><h4 id="for-training-the-sequence-to-sequence-model"><a href="#for-training-the-sequence-to-sequence-model" class="headerlink" title="for training the sequence-to-sequence model"></a>for training the sequence-to-sequence model</h4><p>class config:<br>    #### configuration:结构，布局<br>    AMP &#x3D; True  # Boolean flag indicating whether to use Automatic Mixed Precision (AMP) for training efficiency.<br>    BATCH_SIZE_TRAIN &#x3D; 32 #若出现oom，减少即可<br>    BATCH_SIZE_VALID &#x3D; 32 #若出现oom，减少即可<br>    BETAS &#x3D; (0.9, 0.999)<br>    DEBUG &#x3D; 0 #debug改为1<br>    DEVICE &#x3D; torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)<br>    LR &#x3D; 5e-6<br>    EPOCHS &#x3D; 6<br>    EPS &#x3D; 1e-6<br>    GRADIENT_CHECKPOINTING &#x3D; False<br>    MODEL &#x3D; “&#x2F;kaggle&#x2F;input&#x2F;deberta-v3-large-hf-weights” #模型文件-<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights">https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights</a><br>    CKPT &#x3D; ‘deberta-v3-large’<br>    MAX_GRAD_NORM &#x3D; 100000.0<br>    MAX_LEN &#x3D; 384<br>    NUM_WORKERS &#x3D; 0<br>    PRINT_FREQ &#x3D; 500<br>    SEED &#x3D; 20<br>    WANDB &#x3D; False<br>    WEIGHT_DECAY &#x3D; 0.008</p>
<h4 id="提供一个结构化的方法来管理文件路径"><a href="#提供一个结构化的方法来管理文件路径" class="headerlink" title="提供一个结构化的方法来管理文件路径"></a>提供一个结构化的方法来管理文件路径</h4><p>class paths:<br>    TRAIN_DATA &#x3D; “.&#x2F;train_clean.parquet”<br>    #TRAIN_DATA2 &#x3D; ‘.&#x2F;train_sft_v13.csv’<br>    VALID_DATA &#x3D; ‘.&#x2F;validation826.csv’<br>    train_embedding_file &#x3D; ‘.&#x2F;train_clean_emb_sentence-t5-base.npy’<br>    #train_embedding_file2 &#x3D; ‘.&#x2F;train_sft_v13_emb_sentence-t5-base.npy’<br>    valid_embedding_file &#x3D; ‘.&#x2F;valid826_emb_sentence-t5-base.npy’<br>    OUTPUT_DIR &#x3D; “.&#x2F;exp14”#保存文件夹<br>    LOGGER &#x3D; ‘exp14’</p>
<p>os.makedirs(paths.OUTPUT_DIR, exist_ok&#x3D;True)</p>
<h3 id="该类用于在培训或评估过程中监控和跟踪指标的平均值。"><a href="#该类用于在培训或评估过程中监控和跟踪指标的平均值。" class="headerlink" title="该类用于在培训或评估过程中监控和跟踪指标的平均值。"></a>该类用于在培训或评估过程中监控和跟踪指标的平均值。</h3><p>class AverageMeter(object):<br>    def <strong>init</strong>(self):<br>        self.reset()</p>
<pre><code>def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
</code></pre>
<h3 id="该函数将s改为几min几s的形式"><a href="#该函数将s改为几min几s的形式" class="headerlink" title="该函数将s改为几min几s的形式"></a>该函数将s改为几min几s的形式</h3><p>def asMinutes(s):<br>    m &#x3D; math.floor(s &#x2F; 60)<br>    s -&#x3D; m * 60<br>    return ‘%dm %ds’ % (m, s)</p>
<h3 id="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"><a href="#该函数给出已过去的时间以及根据当前进度预计完成任务所需时间" class="headerlink" title="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"></a>该函数给出已过去的时间以及根据当前进度预计完成任务所需时间</h3><p>def timeSince(since, percent):<br>    now &#x3D; time.time()# 获取当前时间<br>    s &#x3D; now - since# 经过的时间<br>    es &#x3D; s &#x2F; (percent)# es:estimated total time 估计所需总时长<br>    # percent:当前进度<br>    rs &#x3D; es - s# 还要多久完事儿<br>    return ‘%s (remain %s)’ % (asMinutes(s), asMinutes(rs))</p>
<h3 id="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"><a href="#该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用" class="headerlink" title="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"></a>该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用</h3><p>def get_config_dict(config):<br>    config_dict &#x3D; dict((key, value) for key, value in config.<strong>dict</strong>.items()<br>    if not callable(value) and not key.startswith(‘__’))<br>    return config_dict</p>
<h3 id="该函数为model量身定制learning-rates-和-weight-decay"><a href="#该函数为model量身定制learning-rates-和-weight-decay" class="headerlink" title="该函数为model量身定制learning rates 和 weight decay"></a>该函数为model量身定制learning rates 和 weight decay</h3><p>def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay&#x3D;0.0):<br>    param_optimizer &#x3D; list(model.named_parameters())<br>    no_decay &#x3D; [“bias”, “LayerNorm.bias”, “LayerNorm.weight”]<br>    optimizer_parameters &#x3D; [<br>        {‘params’: [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: weight_decay},<br>        {‘params’: [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: 0.0},<br>        {‘params’: [p for n, p in model.named_parameters() if “model” not in n],<br>         ‘lr’: decoder_lr, ‘weight_decay’: 0.0}<br>    ]<br>    return optimizer_parameters</p>
<h4 id="在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency"><a href="#在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency" class="headerlink" title="在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency"></a>在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency</h4><h3 id="get-logger-函数将信息加载到控制台与文件里"><a href="#get-logger-函数将信息加载到控制台与文件里" class="headerlink" title="get_logger()函数将信息加载到控制台与文件里"></a>get_logger()函数将信息加载到控制台与文件里</h3><pre><code>def get_logger(filename=paths.OUTPUT_DIR+&#39;/&#39;+paths.LOGGER):##将信息加载到控制台与文件里，控制台用于实时监控program的进展，而文件用于储存program的进展，相当于存档
from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter
logger = getLogger(__name__)
logger.setLevel(INFO)
handler1 = StreamHandler()
handler1.setFormatter(Formatter(&quot;%(message)s&quot;))
handler2 = FileHandler(filename=f&quot;&#123;filename&#125;.log&quot;)
handler2.setFormatter(Formatter(&quot;%(message)s&quot;))
logger.addHandler(handler1)
logger.addHandler(handler2)
return logger
</code></pre>
<h3 id="seed-everything-为各种随机数生成器-RNG-设置种子"><a href="#seed-everything-为各种随机数生成器-RNG-设置种子" class="headerlink" title="seed_everything()为各种随机数生成器(RNG)设置种子"></a>seed_everything()为各种随机数生成器(RNG)设置种子</h3><pre><code>def seed_everything(seed=20):
random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
</code></pre>
<h3 id="generate-uuid-创建编号"><a href="#generate-uuid-创建编号" class="headerlink" title="generate_uuid()创建编号"></a>generate_uuid()创建编号</h3><p>def generate_uuid():#创建编号，产生随机且独一无二的uuid<br>    return str(uuid.uuid4())</p>
<h3 id="将tensor-dictionary移动到GPU"><a href="#将tensor-dictionary移动到GPU" class="headerlink" title="将tensor dictionary移动到GPU"></a>将tensor dictionary移动到GPU</h3><p>def to_device(inputs, device: str &#x3D; device):#将tensor dictionary移动到GPU<br>    return {k: v.to(device) for k, v in inputs.items()}</p>
<p>LOGGER &#x3D; get_logger()<br>seed_everything(seed&#x3D;config.SEED)</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(config.MODEL)<br>tokenizer.save_pretrained(paths.OUTPUT_DIR + ‘&#x2F;tokenizer&#x2F;‘)</p>
<h3 id="将原始的文本数据转换为表格"><a href="#将原始的文本数据转换为表格" class="headerlink" title="将原始的文本数据转换为表格"></a>将原始的文本数据转换为表格</h3><p>def prepare_input(cfg: type, text: np.ndarray, tokenizer):</p>
<pre><code>inputs = tokenizer.encode_plus(
    text,
    return_tensors=None,
    add_special_tokens=True,
    max_length=cfg.MAX_LEN,
    padding=&#39;max_length&#39;, # TODO: check padding to max sequence in batch
    truncation=True
)
for k, v in inputs.items():
    inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes
return inputs
</code></pre>
<h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/24/2024-5-24/" data-id="clwkf521j0000bgt05eq4g7mo" data-title="2024-5-24" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MATLAB" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/09/MATLAB/" class="article-date">
  <time class="dt-published" datetime="2024-05-09T06:08:16.000Z" itemprop="datePublished">2024-05-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/09/MATLAB/">MATLAB</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近事情好多哦，我现在还在准备数学建模的期末考试，听说老师不出原题了，会很难吧。。。<br>所以来补习一下MATLAB<br>今天距离考试还有6天，等等，六天？就剩六天了？</p>
<p>基础语法：<br>语句后面有分号：运行后不显示运行结果<br>注释：%或快捷键Ctrl+R,Ctrl+T取消注释<br>clear:清除工作区<br>clc:清除命令行窗口的所有文本<br>这俩经常写在最开头   clear;clc<br>disp(‘  ‘):输出函数<br>input(‘ ‘):输入函数<br>a&#x3D;{1 2 3}:向量<br>合并两个字符串：<br>法一：strcat(‘ ‘,’ ‘)<br>法二：{str1,str2}<br>num2str:将数字转换成字符<br>奇怪，心里莫名好难受，学不进去了。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/09/MATLAB/" data-id="clvyukxh70000rwt04j3x4byy" data-title="MATLAB" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-RNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/19/RNN/" class="article-date">
  <time class="dt-published" datetime="2024-04-19T02:26:46.000Z" itemprop="datePublished">2024-04-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/19/RNN/">RNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在看enconder代码的时候发现里面竟然有RNN<br>我还是好好整理一下RNN吧</p>
<p>enconder里的RNN层的作用：<br>实例：（复制chatgpt的，嫌长可以不看）</p>
<ol>
<li>Sentence: “The movie was a disappointment, the acting was terrible.”<br> Processing Words Sequentially: The RNN in the encoder processes the sentence word by word:</li>
</ol>
<p>Word 1: “The”</p>
<p>The RNN receives the embedding vector for “The” and combines it with the initial hidden state (often initialized with zeros).<br>Based on this combination, the RNN updates the hidden state h_1. This initial hidden state doesn’t hold much sentiment information yet.<br>Word 2: “movie”</p>
<p>The RNN receives the embedding vector for “movie” and combines it with the updated hidden state h_1.<br>Now, h_1 contains some context about “The,” which might be neutral. The RNN updates the hidden state h_2 to reflect this context along with the meaning of “movie.”<br>Word 3: “was”</p>
<p>The process repeats. The RNN receives the embedding vector for “was” and combines it with the previous hidden state h_2.<br>h_2 carries information about “The movie,” which could be neutral or slightly positive depending on the embedding vectors. The RNN updates the hidden state h_3 to incorporate the meaning of “was” in this context.<br>Word 4: “a” (similar to “The”)</p>
<p>Word 5: “disappointment”</p>
<p>The RNN receives the embedding vector for “disappointment,” a negative word.<br>The previous hidden state h_4 likely held some neutral or slightly positive sentiment. The RNN updates h_5 to reflect the strong negative connotation of “disappointment,” shifting the overall sentiment towards negative.<br>Words 6-7 follow similarly, likely reinforcing the negative sentiment.</p>
<ol start="3">
<li>Final Hidden State: After processing all words, the RNN in the encoder reaches the final hidden state, let’s call it h_final. This state encapsulates the sentiment of the entire sentence, considering the sequence of words and their relationships.</li>
<li>Using the Hidden State: The h_final can then be used for various tasks:</li>
</ol>
<p>Sentiment Classification: This hidden state can be fed into a classifier (e.g., a simple neural network) to predict the overall sentiment of the sentence (likely negative in this case).<br>Machine Translation: If this is part of a machine translation system, the h_final might be used to encode the meaning of the sentence and generate a corresponding translation in another language.</p>
<p>简而言之，RNN每步读入当前的Word并将其与之前的隐状态结合、更新隐状态，隐状态反映当前输入的所有词之间的relationship<br>在读完整个句子后，我们会得到最后的隐状态，最后的隐状态包含了整句话的所有词之间的relationship<br>最后的隐状态可用于情感分类、机器翻译</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/19/RNN/" data-id="clv61usel0000qgt0gk46hjjd" data-title="RNN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Kaggle-LLM-Science-Exam" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/Kaggle-LLM-Science-Exam/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T06:43:35.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/Kaggle-LLM-Science-Exam/">Kaggle - LLM Science Exam</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>目的：this competition challenges participants to answer difficult science-based questions written by a Large Language Model.<br>任务：预测前三个最可能是正确答案的选项<br>评价标准：Mean Average Precision @ 3 (MAP@3):<br><img src="/image-107.png" alt="alt text"></p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>train集:包括一个问题、五个选项及正确答案<br>test集:是train集的copy,只是没了answer列(啊？这不妥妥过拟合了吗？)</p>
<h2 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h2><h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>选用Embedding模型：gte-base<br>    还有很多其他Embeding方法，如bge-large-en-v1.5\bge-base-en-v1.5\ember-v1\gte-large\e5-large\bge-small<br>不一定哪个表现最好，视具体情况而定<br>可以拿小样本跑一跑看哪个表现好</p>
<p><img src="/image-108.png" alt="alt text"><br>这里有个问题是若使用检索2的话会使Prompt占的比重很小</p>
<p>一些tricks:若GPU性能没那么好，就调小batch_size(1或2)<br>若改成1还跑不了就调小max_input</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/Kaggle-LLM-Science-Exam/" data-id="clv3g5d5p0000bgt0a3la0wud" data-title="Kaggle - LLM Science Exam" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle—HMS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/kaggle%E2%80%94HMS/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T00:57:30.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/kaggle%E2%80%94HMS/">kaggle—HMS</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>兜兜转转犹犹豫豫最后还是在淘宝上买了kaggle竞赛的课<br>花了我3600大洋呜呜呜呜，不过可以保证我拿银牌，我心里就有底了</p>
<p>那么废话不多说，第一课讲的是HMS</p>
<p>打比赛的流程：<br><img src="/image-100.png" alt="alt text"></p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>比赛目的：检测分类癫痫发作和其他类型的有害大脑活动。<br>比赛任务：开发一个model,根据从医院重症患者记录的脑电图(EEG)信号进行训练。<br>评价指标：KL散度(不常用)</p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>data:<br>这数据有啥特征啊。。。</p>
<p><img src="/image-103.png" alt="alt text"><br><img src="/image-101.png" alt="alt text"><br>train数据集包括50秒的EEG(相应匹配于频谱图)<br>[seizure&#x2F;lpd&#x2F;gpd&#x2F;lrda&#x2F;grda&#x2F;other]_vote：<br>人工打分的各类疾病投票<br>一共有6个label，分别是seizure(癫痫)、GRDA、Other、GPD、LRDA、LPD<br>(查不到都是啥病)</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>用librosa包进行特征提取</p>
<h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p><img src="/image-102.png" alt="alt text"><br><img src="/image-104.png" alt="alt text"><br>方案一直接用给的数据<br><img src="/image-105.png" alt="alt text"><br>方案二用提取特征后的<br><img src="/image-106.png" alt="alt text"><br>方案三用给的+提取特征后的(效果最好)</p>
<p>提分要点：使用vote&gt;10的样本二次训练<br>(即第一次用全部的train集训练，第二次用vote高的进行训练，因为vote高代表准确率更高，但即便如此，vote少的依然可以提供一些信息(钱少也是钱啊))</p>
<p>大致的思路我懂了，但是没怎么看代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/kaggle%E2%80%94HMS/" data-id="clv33sa2q0000ekt0akf20t8c" data-title="kaggle—HMS" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-prompt" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/13/prompt/" class="article-date">
  <time class="dt-published" datetime="2024-04-13T01:49:55.000Z" itemprop="datePublished">2024-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/13/prompt/">prompt</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近在做大创，是关于数字人的，这个项目我很感兴趣，感觉未来可以应用的范围很广，前景也很好。<br>等以后元宇宙火起来了感觉这个会很赚钱</p>
<p>好了回到正题<br><img src="/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240413094011.jpg" alt="alt text"><br>这是我们模型的框架</p>
<h2 id="关于prompt"><a href="#关于prompt" class="headerlink" title="关于prompt:"></a>关于prompt:</h2><p>即提示词(咒语)，是LLM(large language model)的重要起点</p>
<h2 id="RAG检索"><a href="#RAG检索" class="headerlink" title="RAG检索"></a>RAG检索</h2><p>(Retrieval Augmented Generation)<br>结合prompt和其他相关资料，使得LLM的能力更上一层楼<br>RAG可用于回答问题、总结、fact checking、对话系统</p>
<h2 id="多模态融合"><a href="#多模态融合" class="headerlink" title="多模态融合"></a>多模态融合</h2><p>深度学习多模态融合指机器从文本、图像、语音、视频等多个领域获取信息</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/13/prompt/" data-id="cluxfwb0g00009ot0h4bpfvad" data-title="prompt" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
          </li>
        
          <li>
            <a href="/2024/07/03/metaverse/">metaverse</a>
          </li>
        
          <li>
            <a href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a>
          </li>
        
          <li>
            <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
          </li>
        
          <li>
            <a href="/2024/05/24/2024-5-24/">2024-5-24</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>