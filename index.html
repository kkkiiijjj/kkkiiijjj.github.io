<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Forforevery">
<meta property="og:url" content="https://kkkiiijjj.github.io/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Cursor使用教程" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/13/Cursor%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/" class="article-date">
  <time class="dt-published" datetime="2024-12-13T11:30:56.000Z" itemprop="datePublished">2024-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/13/Cursor%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">Cursor使用教程</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/12/13/Cursor%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/" data-id="cm4mo3c1y0000o0t0e3puf6ia" data-title="Cursor使用教程" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-llm项目解析-桌宠" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/13/llm%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90-%E6%A1%8C%E5%AE%A0/" class="article-date">
  <time class="dt-published" datetime="2024-12-12T19:45:08.000Z" itemprop="datePublished">2024-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/13/llm%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90-%E6%A1%8C%E5%AE%A0/">llm项目解析-桌宠</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>整个压缩包里需要读的代码基本就两个文件：main.py和talk_show.py</p>
<h1 id="阅读readme文件"><a href="#阅读readme文件" class="headerlink" title="阅读readme文件"></a>阅读readme文件</h1><p>得知基本功能是</p>
<ol>
<li>显示动图和文字</li>
<li>大模型生成文本</li>
<li>休息一下，隔一小时提醒休息</li>
</ol>
<h1 id="试着运行程序"><a href="#试着运行程序" class="headerlink" title="试着运行程序"></a>试着运行程序</h1><p>tm一直报错，qtmd我要自己写了</p>
<p>使用了huggingface里的模型<br>学到了怎么从huggingface里导入模型<br><img src="/image-119.png" alt="alt text"><br>这里导入的pipeline包据说是简化复杂文本生成任务的包</p>
<h1 id="读main文件"><a href="#读main文件" class="headerlink" title="读main文件"></a>读main文件</h1><p>吓到了，总共300+行代码。。。<br>不知道从哪开始看了<br>折叠了一下，发现怎么就是一个DesktopPet类加一段运行用的代码啊<br>然后里面写了十几个函数分别实现一些功能</p>
<ol>
<li><strong>init</strong>(self, parent&#x3D;None, **kwargs)构造函数</li>
<li>init(self)实现初始化</li>
<li>initPall(self)实现托盘初始化</li>
<li>initPetImage(self)实现宠物静态gif图加载</li>
<li>petNormalAction(self)宠物正常待机动作</li>
<li>randomAct(self)随机动作切换</li>
<li>talk(self)宠物对话框行为处理</li>
<li>quit(self)退出操作，关闭程序</li>
<li>showwin(self)显示宠物</li>
<li>randomPosition(self)宠物随机位置</li>
<li>mousePressEvent(self, event)鼠标左键按下时, 宠物将和鼠标位置绑定</li>
<li>mouseMoveEvent(self, event)鼠标移动时调用，实现宠物随鼠标移动</li>
<li>mouseReleaseEvent(self, event)鼠标释放调用，取消绑定</li>
<li>enterEvent(self, event)鼠标移进时调用</li>
<li>contextMenuEvent(self, event)宠物右键点击交互</li>
<li>haveRest(self)休息时间<br>核心功能的实现主要靠4、5、6、7、9、15、16</li>
</ol>
<p>2.初始化窗口用PyQt5(待了解)</p>
<p>这边我打算自己写一个桌宠了<br>去问了一下chatgpt怎么写一个桌宠程序，他给我推荐了以下方案：</p>
<ol>
<li>Python +   PyQt</li>
<li>C++    +   Qt</li>
<li>Java   +   JavaFx<br>核心功能：</li>
<li>创造一个窗口显示宠物</li>
<li>加载宠物图像</li>
<li>优化宠物行为比如移动、与用户互动</li>
</ol>
<p>查了一下AI助写工具，有Cursor、Copilot、Claude<br>这边新开一篇笔记讲Cursor</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/12/13/llm%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90-%E6%A1%8C%E5%AE%A0/" data-id="cm4lqb2te0000zkt0fofr889f" data-title="llm项目解析-桌宠" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-大模型小项目解析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2024-12-08T12:14:04.000Z" itemprop="datePublished">2024-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90/">大模型小项目解析</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这个项目是从github上搜到的，一个比较简单的应用到了大语言模型的项目<br>实现了从键盘获取用户输入并传给大模型进行分析与回复，主要功能是天气查询<br>读了.md文件配置好环境之后发现它载入的大模型好大个，占我内存，不悦<br>而且回复很慢，不知道为啥</p>
<p>首先分析一下代码</p>
<h1 id="大致结构"><a href="#大致结构" class="headerlink" title="大致结构"></a>大致结构</h1><p>(可以去语雀看大体框架)</p>
<h2 id="从main函数开始看"><a href="#从main函数开始看" class="headerlink" title="从main函数开始看"></a>从main函数开始看</h2><ol>
<li>首先提示用户按下w键进入文字模式</li>
<li>然后无限循环判断用户是否按下了W键，如果按下，那么跳出循环</li>
<li>进入下一个循环，提示用户输入，并将输入的内容存储在 user_input 变量中。<br>如果输入不是退出则运行函数send_message_to_ollama</li>
<li>往上翻查看send_message_to_ollama函数的定义：该函数的参数为用户当前输入及上下文<br>调用该函数会用到langchain库初始化一个agent，（from langchain.agents import initialize_agent, Tool, AgentType）<br>运行这个Agent（以prompt_value为参数，prompt_value包含了用户的输入、上下文（也是该函数的参数）以及agent的设定prompt）<br>并返回结果</li>
<li>回看main函数，结果被保存进response里了</li>
<li>对response进行再加工，用正则表达式去除尾部空格</li>
<li>显示结果并更新上下文</li>
</ol>
<h1 id="详细解读"><a href="#详细解读" class="headerlink" title="详细解读"></a>详细解读</h1><h2 id="异步函数"><a href="#异步函数" class="headerlink" title="异步函数"></a>异步函数</h2><p>main函数是异步函数，异步函数通常用于非阻塞操作，比如等待用户输入<br>由于使用了 async&#x2F;await 语法，需要在运行时使用 asyncio 模块。例如：<br>import asyncio<br>asyncio.run(main())</p>
<h2 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h2><p>while True:创建一个无限循环，不断检查用户的键盘输入。</p>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>try-except块 用于捕获可能发生的异常，并进行相应的处理。</p>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p> re.sub(r’\s+$’, ‘’, response)<br> r代表represent<br> $代表结尾<br> \s匹配一个空格符号<br>\s+匹配一或多个空格<br>合起来就是若结尾有一或多个空格则用一个空格代替</p>
<h2 id="f-string："><a href="#f-string：" class="headerlink" title="f-string："></a>f-string：</h2><p>print(f”羽汐: {response}”)</p>
<p>Python 3.6 及以上版本引入的一种字符串格式化方式，以 f 开头。<br>在 f-string 中，可以使用花括号 {} 将变量或表达式嵌入到字符串中<br>其中{response}是占位符</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/12/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90/" data-id="cm4fkfl79000028t02kcahcbi" data-title="大模型小项目解析" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-llm训练方法概述" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/15/llm%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/" class="article-date">
  <time class="dt-published" datetime="2024-08-15T14:27:13.000Z" itemprop="datePublished">2024-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/15/llm%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/">llm训练方法概述</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>LLM的训练方法：一个综合概述<br>大型语言模型（LLM）的训练是一项复杂且资源密集型的任务，涉及大量的数据和计算资源。下面将详细介绍LLM训练的常见方法和关键步骤：</p>
<ol>
<li>数据准备<br>数据收集: 收集高质量、多样化的文本数据，如书籍、文章、代码、对话等。<br>数据清洗: 处理数据中的噪声、错误和不一致性。<br>数据预处理: 分词、词向量化、构建训练样本等。</li>
<li>模型选择与架构<br>模型架构: 选择适合的LLM架构，如Transformer、BERT、GPT等。<br>参数设置: 调整模型的超参数，如层数、隐藏层大小、注意力头数等。</li>
<li>训练过程<br>损失函数: 定义合适的损失函数，如交叉熵损失，来衡量模型预测与真实标签之间的差异。<br>优化器: 选择合适的优化器，如Adam、SGD等，来更新模型参数。<br>训练循环: 反复迭代训练数据，计算损失，更新参数。<br>正则化: 使用正则化技术，如Dropout、L1&#x2F;L2正则化，防止过拟合。</li>
<li>微调（Fine-tuning）<br>特定任务: 将预训练好的LLM适配到特定任务，如问答、文本生成、情感分析等。<br>少量数据: 使用较少标注数据进行微调，提高模型在特定任务上的性能。</li>
<li>评估<br>评估指标: 选择合适的评估指标，如困惑度、准确率、F1值等，来评估模型的性能。<br>测试集: 在独立的测试集上评估模型的泛化能力。<br>常见训练方法<br>自监督学习:<br>语言模型预训练: 通过预测下一个单词来学习语言的统计规律。<br>掩码语言模型（MLM）: 随机掩盖输入文本中的部分单词，让模型预测被掩盖的单词。<br>下一个句子预测（NSP）: 判断两个句子是否连贯。<br>监督学习:<br>序列标注: 给定输入序列，预测每个元素的标签。<br>文本分类: 将文本分为不同的类别。<br>机器翻译: 将一种语言的文本翻译成另一种语言。<br>强化学习:<br>策略梯度: 通过奖励机制来训练模型，使其在特定任务上取得更好的表现。<br>挑战与未来方向<br>计算资源: LLM训练需要大量的计算资源，如GPU、TPU。<br>数据质量: 高质量的数据是训练LLM的关键。<br>模型可解释性: 理解LLM的决策过程是重要的研究方向。<br>偏见与公平性: 训练数据中的偏见可能导致模型产生歧视性的输出。<br>总结<br>LLM的训练是一个不断发展和完善的过程。随着技术的进步，我们将会看到越来越强大和智能的语言模型。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/08/15/llm%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/" data-id="clzvdju130000hst0by1bg88c" data-title="llm训练方法概述" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-llm应用开发" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/" class="article-date">
  <time class="dt-published" datetime="2024-08-01T10:37:08.000Z" itemprop="datePublished">2024-08-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="llm应用开发日志"><a href="#llm应用开发日志" class="headerlink" title="llm应用开发日志"></a>llm应用开发日志</h1><h2 id="大致顺序"><a href="#大致顺序" class="headerlink" title="大致顺序"></a>大致顺序</h2><ol>
<li>项目规划&amp;需求分析</li>
<li>数据准备</li>
<li>明确框架</li>
<li>按照框架进行填充</li>
<li>测试</li>
<li>维护</li>
</ol>
<h3 id="LangChain框架"><a href="#LangChain框架" class="headerlink" title="LangChain框架"></a>LangChain框架</h3><p>用途：方便开发者搭建LLM应用<br>模块：</p>
<ol>
<li>Memory</li>
<li>Chain</li>
<li>Agents</li>
<li>Modules</li>
<li>Prompt</li>
<li>Callbacks<br>用例：</li>
<li>文档问答</li>
<li>私人助理</li>
<li>与API交互</li>
<li>信息提取<br>总之这个很适合做一些个性化私人化的应用</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/" data-id="clzb561dr0000qkt051l00u13" data-title="llm应用开发" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-metaverse" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/07/03/metaverse/" class="article-date">
  <time class="dt-published" datetime="2024-07-03T02:40:03.000Z" itemprop="datePublished">2024-07-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/07/03/metaverse/">metaverse</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在规划我的未来路线时忽然想起之前很火的一个名词————metaverse<br>然后想想估计很有前景而且我也很感兴趣</p>
<p>于是去搜了几篇论文，也不知道我这种查找资料的方式对不对<br>论文链接：<br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=ljHcwhvF5s0D3XXrGK85BedHtACuRUDSSU5Pzjly8BimR92nVbEF04PeMa89ciBdiAB3f/dEeS5LX/yF0UnUuvO8/ACxlT4pqLwuUtTvHHVh1HKkl99dbeirtmqj1HWo+g5ajRIakcVqZuR5kQtP+eGP/L71SGQKiGkdQ6IJQtY=&DBCODE=CJFQ&FileName=NEWS202110008&TABLEName=cjfdlast2021&nonce=FE8BE552A6D94C34BF2260AEB757A31A&TIMESTAMP=1719977974607&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=ljHcwhvF5s0D3XXrGK85BedHtACuRUDSSU5Pzjly8BimR92nVbEF04PeMa89ciBdiAB3f%2FdEeS5LX%2FyF0UnUuvO8%2FACxlT4pqLwuUtTvHHVh1HKkl99dbeirtmqj1HWo%2Bg5ajRIakcVqZuR5kQtP%2BeGP%2FL71SGQKiGkdQ6IJQtY%3D&amp;DBCODE=CJFQ&amp;FileName=NEWS202110008&amp;TABLEName=cjfdlast2021&amp;nonce=FE8BE552A6D94C34BF2260AEB757A31A&amp;TIMESTAMP=1719977974607&amp;uid=</a><br>(这篇是被引最多的)<br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=nd8tH6Gp8qusAUBJeVilhVKqvQpVV84H437QH2hJqpnLJVit7CkDvQbXpZfDbocpF0SzafPlOqDs4rZBqOoqQMhH36JjrKYhgA9Vsx8UGVvANv/AkazvvwbcxmIczDk8icEYpamtJvYLnBYggJ/cJkbK3agFnW+BPlsMIU6YJ8g=&DBCODE=CJFQ&FileName=XJSF202203010&TABLEName=cjfdlast2022&nonce=3CA8110B039B40C6B82153C4D3B59369&TIMESTAMP=1719803796798&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=nd8tH6Gp8qusAUBJeVilhVKqvQpVV84H437QH2hJqpnLJVit7CkDvQbXpZfDbocpF0SzafPlOqDs4rZBqOoqQMhH36JjrKYhgA9Vsx8UGVvANv%2FAkazvvwbcxmIczDk8icEYpamtJvYLnBYggJ%2FcJkbK3agFnW%2BBPlsMIU6YJ8g%3D&amp;DBCODE=CJFQ&amp;FileName=XJSF202203010&amp;TABLEName=cjfdlast2022&amp;nonce=3CA8110B039B40C6B82153C4D3B59369&amp;TIMESTAMP=1719803796798&amp;uid=</a><br><a target="_blank" rel="noopener" href="https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=oLlOB/q0RPqvHaS5NBtXuIIwWfHV2gPO7AiDUhskGTfocKKzHaVlvmPHQhUuWrLs7GpX4mt5oV0bmscV2/W0Dqu1N4n79tghMBrFGWm8KUr7xH1qtw3cRig163LJR4HSER8FYSabbMEsoKuO9OijzLTdDJ7eOssK4MymxjSM1QE=&DBCODE=CJFQ&FileName=XNZY202204002&TABLEName=cjfdlast2022&nonce=D3C7584445A74505835C153C544F2E1F&TIMESTAMP=1719804554876&uid=">https://webvpn.hfut.edu.cn/https/77726476706e69737468656265737421fbf952d2243e635930068cb8/KXReader/Detail?invoice=oLlOB%2Fq0RPqvHaS5NBtXuIIwWfHV2gPO7AiDUhskGTfocKKzHaVlvmPHQhUuWrLs7GpX4mt5oV0bmscV2%2FW0Dqu1N4n79tghMBrFGWm8KUr7xH1qtw3cRig163LJR4HSER8FYSabbMEsoKuO9OijzLTdDJ7eOssK4MymxjSM1QE%3D&amp;DBCODE=CJFQ&amp;FileName=XNZY202204002&amp;TABLEName=cjfdlast2022&amp;nonce=D3C7584445A74505835C153C544F2E1F&amp;TIMESTAMP=1719804554876&amp;uid=</a></p>
<p>来源：小说《雪崩》<br>已有相关部署的公司：微软、Facebook、Roblox(元宇宙第一股)<br>国内的有字节、腾讯、网易、莉莉丝、米哈游、中青宝等游戏公司<br>中心技术：区块链、交互技术、电子游戏技术、人工智能技术、智能网络技术、物联网技术<br>区块链：是实现去中心化的分布式社会中人与人之间信任、协同的技术基础<br>包含要素：Roblox的首席执行官大卫.巴斯祖基认为至少包括身份、朋友、沉浸感、低延迟、多元化、随地、经济系统和文明要素<br>突破点：实现人的嗅觉、味觉及触觉等感官效应的线上化<br>终端要足够便携、易佩戴（“瘦终端、胖云端”）<br>VR、AR、MR等扩展现实<br><img src="/image-116.png" alt="alt text"><br><img src="/image-117.png" alt="alt text"><br><img src="/image-118.png" alt="alt text"><br>基本特征：去中心化<br>四大核心属性：<br>1.与现实世界的同步性与高拟真度。元宇宙虚拟空间与现实社会保持高度同步和互通，交互效果逼近真实。具有同步性和高拟真度的虚拟世界是元宇宙构成的基础条件，它意味着现实社会中发生的一切事件将同步于虚拟世界，同时用户在虚拟的元宇宙中进行交互时能得到近乎真实的反馈信息。<br>2.开源开放与创新创造。开源开放是指技术开源和平台开源，元宇宙通过制定“标准”和“协议”将代码进行不同程度的封装<br>和模块化，不同需求的用户都可以在元宇宙进行自主创新和创造，构建原创的虚拟世界，不断拓展元宇宙边界。<br>3.永续发展。元宇宙平台的建设和发展不会“暂停 或“结束”，而是以开源开放的方式运行并无限期地持续发展.<br>4.拥有闭环运行的经济系统。在元字宙申，用户的生产和工作活动的价值将以平台统一的货币形式被确认和确权，用户可以使用这一货币在元宇宙平台内进行消费，也可以通过一定比例“兑换”成现实生活中的法定货币。毫无疑问，经济系统的闭环运行是驱动和保障元宇宙不断变化和发展的动力引擎。</p>
<p>建议朗读全文：<br>《2022胡润中国元宇宙潜力企业榜》<br><a target="_blank" rel="noopener" href="http://www.pjtime.com/2022/6/322149147125.shtml#:~:text=%E8%83%A1%E6%B6%A6%E7%A0%94%E7%A9%B6%E9%99%A22022%E5%B9%B46%E6%9C%8815%E6%97%A5%E4%BA%8E%E5%B9%BF%E5%B7%9E%E5%8D%97%E6%B2%99%E5%8F%91%E5%B8%83%E3%80%8A2022%E8%83%A1%E6%B6%A6%E4%B8%AD%E5%9B%BD%E5%85%83%E5%AE%87%E5%AE%99%E6%BD%9C%E5%8A%9B%E4%BC%81%E4%B8%9A%E6%A6%9C%E3%80%8B%EF%BC%88Hurun%20China%20Metaverse%20Companies,with%20the%20Greatest%20Potential%202022%EF%BC%89%EF%BC%8C%E5%88%97%E5%87%BA%E4%BA%86%E5%85%83%E5%AE%87%E5%AE%99%E9%A2%86%E5%9F%9F%E6%9C%80%E5%85%B7%E5%8F%91%E5%B1%95%E6%BD%9C%E5%8A%9B%E7%9A%84%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A200%E5%BC%BA%EF%BC%8C%E5%88%86%E4%B8%BA%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9C%80%E5%85%B7%E6%BD%9C%E5%8A%9B20%E5%BC%BA%E3%80%8150%E5%BC%BA%E3%80%81100%E5%BC%BA%E5%92%8C200%E5%BC%BA%E3%80%82">http://www.pjtime.com/2022/6/322149147125.shtml#:~:text=%E8%83%A1%E6%B6%A6%E7%A0%94%E7%A9%B6%E9%99%A22022%E5%B9%B46%E6%9C%8815%E6%97%A5%E4%BA%8E%E5%B9%BF%E5%B7%9E%E5%8D%97%E6%B2%99%E5%8F%91%E5%B8%83%E3%80%8A2022%E8%83%A1%E6%B6%A6%E4%B8%AD%E5%9B%BD%E5%85%83%E5%AE%87%E5%AE%99%E6%BD%9C%E5%8A%9B%E4%BC%81%E4%B8%9A%E6%A6%9C%E3%80%8B%EF%BC%88Hurun%20China%20Metaverse%20Companies,with%20the%20Greatest%20Potential%202022%EF%BC%89%EF%BC%8C%E5%88%97%E5%87%BA%E4%BA%86%E5%85%83%E5%AE%87%E5%AE%99%E9%A2%86%E5%9F%9F%E6%9C%80%E5%85%B7%E5%8F%91%E5%B1%95%E6%BD%9C%E5%8A%9B%E7%9A%84%E4%B8%AD%E5%9B%BD%E4%BC%81%E4%B8%9A200%E5%BC%BA%EF%BC%8C%E5%88%86%E4%B8%BA%E5%9B%9B%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9C%80%E5%85%B7%E6%BD%9C%E5%8A%9B20%E5%BC%BA%E3%80%8150%E5%BC%BA%E3%80%81100%E5%BC%BA%E5%92%8C200%E5%BC%BA%E3%80%82</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/07/03/metaverse/" data-id="cly58cryc0000q0t0h6za5qnu" data-title="metaverse" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-注意力机制" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="article-date">
  <time class="dt-published" datetime="2024-06-24T01:37:03.000Z" itemprop="datePublished">2024-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>废话：我发现看电子版的书有一个弊端，就是容易熊瞎子匹苞米，看了后面的就忘了前面的，每个章节之间的联系不清楚，缺乏整体的一个总览。可能人类已经习惯阅读纸质书了</p>
<h2 id="注意力提示"><a href="#注意力提示" class="headerlink" title="注意力提示"></a>注意力提示</h2><h3 id="人的注意力"><a href="#人的注意力" class="headerlink" title="人的注意力"></a>人的注意力</h3><p>主要包括  自主性的  与  非自主性的  注意力</p>
<h3 id="查询、键和值-query-key-and-value"><a href="#查询、键和值-query-key-and-value" class="headerlink" title="查询、键和值(query,key and value)"></a>查询、键和值(query,key and value)</h3><p><img src="/image-113.png" alt="alt text"><br>个人理解就是key是外在物品的特征，而query是个人的内在特征<br>值是可供选择的事物</p>
<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3><p>Nadaraya-Watson核回归（Nadaraya-Watson kernel regression）:根据输入的位置对输出y_i进行加权<br>注意力汇聚（attention pooling）公式：<br><img src="/image-114.png" alt="alt text"><br>其中x是查询，$(x_i,y_i)$是键值对<br>x和key$x_i$之间的关系建模为注意力权重（attention weight）:α(x,$x_i$)<br>非参数的Nadaraya-Watson核回归具有一致性（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 </p>
<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3><p>在下面的查询x和键$x_i$之间的距离乘以可学习参数w<br><img src="/image-115.png" alt="alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/06/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" data-id="clxsb52zg0000ykt04sz46xoe" data-title="注意力机制" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle竞赛——AI-Mathematical" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" class="article-date">
  <time class="dt-published" datetime="2024-06-22T06:41:17.000Z" itemprop="datePublished">2024-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>OK,这是我参加的第一个kaggle比赛<br>还花钱报了班<br>比赛6月28出成绩结果到现在我一个代码都还没敲(汗)<br>比赛传送门：<a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/overview">https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/overview</a></p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>AI Mathematical Olympiad - Progress Prize 1<br>目的：创建算法和model解决困难的数学问题(问题是LaTex格式的)，旨在帮助AI模型提高数学逻辑推理能力<br>任务：给出test集的数学题的答案<br>评价标准：预测值的准确度</p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>train集：包含十个问题及其答案的.csv文件<br>test集：说是给了50个问题，但是我只看到了3个</p>
<h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>好像很多人用的Gemma</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" data-id="clxpr4tyx00009wt0dmo3hmks" data-title="kaggle竞赛——AI Mathematical" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-5-24" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/24/2024-5-24/" class="article-date">
  <time class="dt-published" datetime="2024-05-24T08:27:09.000Z" itemprop="datePublished">2024-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/24/2024-5-24/">2024-5-24</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一些废话：在看kaggle竞赛的课<br>感觉讲的不是很行啊，听半天感觉和没听一样</p>
<p>竞赛：LLM Prompt Recovery | Metric Computation<br>比赛地址直达：<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation">https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation</a><br>目的：由原始文本与Gemma改写的文本段落反推提示词(prompt)<br>评价指标：<br>锐化余弦相似度<img src="/image-112.png" alt="alt text"><br>可以用三个不同的模型来解决</p>
<ol>
<li>Seq2Seq</li>
<li>few-shot<br>zero-shot LLM       没有示例<br>one-shot LLM        给一个示例<br>few-shot LLM        给一些示例</li>
<li>Phi 2</li>
</ol>
<p>又是一段废话：<br>然后好像没有有用的话了，code给的还特别小，是想让我瞎吗()<br>好在可以管客服索要代码，看网课不如看代码<br>代码老长老长，给的是.ipynb格式的文件，据说要用juypter打开，不过我用vscode也打开了。<br>觉得自己有必要抽空学一下juypter</p>
<p>代码在这里放一下吧还是</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>%%writefile trian_embedding_generate.py<br>import pandas as pd<br>import gc<br>import numpy as np<br>import pandas as pd<br>import time<br>from tqdm import tqdm<br>import numpy as np<br>from sentence_transformers import SentenceTransformer<br>import pickle</p>
<p>df &#x3D; pd.read_parquet(f”.&#x2F;train_clean.parquet”, columns&#x3D;[‘rewrite_prompt’])</p>
<h4 id="这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下"><a href="#这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下" class="headerlink" title="这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)"></a>这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)</h4><p>valid &#x3D; pd.read_csv(‘.&#x2F;validation826.csv’, usecols&#x3D;[‘rewrite_prompt’])</p>
<h4 id="两个文件均只导入特定列"><a href="#两个文件均只导入特定列" class="headerlink" title="两个文件均只导入特定列"></a>两个文件均只导入特定列</h4><p>model &#x3D;  SentenceTransformer(‘sentence-transformers&#x2F;sentence-t5-base’)#</p>
<h4 id="选择SentenceTransformer-model进行embedding"><a href="#选择SentenceTransformer-model进行embedding" class="headerlink" title="选择SentenceTransformer model进行embedding"></a>选择SentenceTransformer model进行embedding</h4><p>model.max_seq_length &#x3D; 512</p>
<h4 id="最长序列长度设为512"><a href="#最长序列长度设为512" class="headerlink" title="最长序列长度设为512"></a>最长序列长度设为512</h4><p>encoded_data &#x3D; model.encode(list(df[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>encoded_data &#x3D; encoded_data.detach().cpu().numpy()<br>encoded_data &#x3D; np.asarray(encoded_data.astype(‘float32’))</p>
<p>np.save(‘train_clean_emb_sentence-t5-base.npy’, encoded_data)</p>
<p>valid_emb &#x3D; model.encode(list(valid[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>valid_emb &#x3D; valid_emb.detach().cpu().numpy()<br>valid_emb &#x3D; np.asarray(valid_emb.astype(‘float32’))</p>
<p>np.save(‘valid826_emb_sentence-t5-base.npy’, valid_emb)</p>
<h4 id="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里"><a href="#这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里" class="headerlink" title="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里"></a>这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里</h4><h1 id="seq2seq训练"><a href="#seq2seq训练" class="headerlink" title="seq2seq训练"></a>seq2seq训练</h1><h4 id="the-config-class-provides-a-structured-way-to"><a href="#the-config-class-provides-a-structured-way-to" class="headerlink" title="the config class provides a structured way to"></a>the config class provides a structured way to</h4><h4 id="define-and-manage-the-various-hyperparameters-and-configuration-settings"><a href="#define-and-manage-the-various-hyperparameters-and-configuration-settings" class="headerlink" title="define and manage the various hyperparameters and configuration settings"></a>define and manage the various hyperparameters and configuration settings</h4><h4 id="for-training-the-sequence-to-sequence-model"><a href="#for-training-the-sequence-to-sequence-model" class="headerlink" title="for training the sequence-to-sequence model"></a>for training the sequence-to-sequence model</h4><p>class config:<br>    #### configuration:结构，布局<br>    AMP &#x3D; True  # Boolean flag indicating whether to use Automatic Mixed Precision (AMP) for training efficiency.<br>    BATCH_SIZE_TRAIN &#x3D; 32 #若出现oom，减少即可<br>    BATCH_SIZE_VALID &#x3D; 32 #若出现oom，减少即可<br>    BETAS &#x3D; (0.9, 0.999)<br>    DEBUG &#x3D; 0 #debug改为1<br>    DEVICE &#x3D; torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)<br>    LR &#x3D; 5e-6<br>    EPOCHS &#x3D; 6<br>    EPS &#x3D; 1e-6<br>    GRADIENT_CHECKPOINTING &#x3D; False<br>    MODEL &#x3D; “&#x2F;kaggle&#x2F;input&#x2F;deberta-v3-large-hf-weights” #模型文件-<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights">https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights</a><br>    CKPT &#x3D; ‘deberta-v3-large’<br>    MAX_GRAD_NORM &#x3D; 100000.0<br>    MAX_LEN &#x3D; 384<br>    NUM_WORKERS &#x3D; 0<br>    PRINT_FREQ &#x3D; 500<br>    SEED &#x3D; 20<br>    WANDB &#x3D; False<br>    WEIGHT_DECAY &#x3D; 0.008</p>
<h4 id="提供一个结构化的方法来管理文件路径"><a href="#提供一个结构化的方法来管理文件路径" class="headerlink" title="提供一个结构化的方法来管理文件路径"></a>提供一个结构化的方法来管理文件路径</h4><p>class paths:<br>    TRAIN_DATA &#x3D; “.&#x2F;train_clean.parquet”<br>    #TRAIN_DATA2 &#x3D; ‘.&#x2F;train_sft_v13.csv’<br>    VALID_DATA &#x3D; ‘.&#x2F;validation826.csv’<br>    train_embedding_file &#x3D; ‘.&#x2F;train_clean_emb_sentence-t5-base.npy’<br>    #train_embedding_file2 &#x3D; ‘.&#x2F;train_sft_v13_emb_sentence-t5-base.npy’<br>    valid_embedding_file &#x3D; ‘.&#x2F;valid826_emb_sentence-t5-base.npy’<br>    OUTPUT_DIR &#x3D; “.&#x2F;exp14”#保存文件夹<br>    LOGGER &#x3D; ‘exp14’</p>
<p>os.makedirs(paths.OUTPUT_DIR, exist_ok&#x3D;True)</p>
<h3 id="该类用于在培训或评估过程中监控和跟踪指标的平均值。"><a href="#该类用于在培训或评估过程中监控和跟踪指标的平均值。" class="headerlink" title="该类用于在培训或评估过程中监控和跟踪指标的平均值。"></a>该类用于在培训或评估过程中监控和跟踪指标的平均值。</h3><p>class AverageMeter(object):<br>    def <strong>init</strong>(self):<br>        self.reset()</p>
<pre><code>def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
</code></pre>
<h3 id="该函数将s改为几min几s的形式"><a href="#该函数将s改为几min几s的形式" class="headerlink" title="该函数将s改为几min几s的形式"></a>该函数将s改为几min几s的形式</h3><p>def asMinutes(s):<br>    m &#x3D; math.floor(s &#x2F; 60)<br>    s -&#x3D; m * 60<br>    return ‘%dm %ds’ % (m, s)</p>
<h3 id="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"><a href="#该函数给出已过去的时间以及根据当前进度预计完成任务所需时间" class="headerlink" title="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"></a>该函数给出已过去的时间以及根据当前进度预计完成任务所需时间</h3><p>def timeSince(since, percent):<br>    now &#x3D; time.time()# 获取当前时间<br>    s &#x3D; now - since# 经过的时间<br>    es &#x3D; s &#x2F; (percent)# es:estimated total time 估计所需总时长<br>    # percent:当前进度<br>    rs &#x3D; es - s# 还要多久完事儿<br>    return ‘%s (remain %s)’ % (asMinutes(s), asMinutes(rs))</p>
<h3 id="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"><a href="#该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用" class="headerlink" title="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"></a>该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用</h3><p>def get_config_dict(config):<br>    config_dict &#x3D; dict((key, value) for key, value in config.<strong>dict</strong>.items()<br>    if not callable(value) and not key.startswith(‘__’))<br>    return config_dict</p>
<h3 id="该函数为model量身定制learning-rates-和-weight-decay"><a href="#该函数为model量身定制learning-rates-和-weight-decay" class="headerlink" title="该函数为model量身定制learning rates 和 weight decay"></a>该函数为model量身定制learning rates 和 weight decay</h3><p>def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay&#x3D;0.0):<br>    param_optimizer &#x3D; list(model.named_parameters())<br>    no_decay &#x3D; [“bias”, “LayerNorm.bias”, “LayerNorm.weight”]<br>    optimizer_parameters &#x3D; [<br>        {‘params’: [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: weight_decay},<br>        {‘params’: [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: 0.0},<br>        {‘params’: [p for n, p in model.named_parameters() if “model” not in n],<br>         ‘lr’: decoder_lr, ‘weight_decay’: 0.0}<br>    ]<br>    return optimizer_parameters</p>
<h4 id="在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency"><a href="#在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency" class="headerlink" title="在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency"></a>在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency</h4><h3 id="get-logger-函数将信息加载到控制台与文件里"><a href="#get-logger-函数将信息加载到控制台与文件里" class="headerlink" title="get_logger()函数将信息加载到控制台与文件里"></a>get_logger()函数将信息加载到控制台与文件里</h3><pre><code>def get_logger(filename=paths.OUTPUT_DIR+&#39;/&#39;+paths.LOGGER):##将信息加载到控制台与文件里，控制台用于实时监控program的进展，而文件用于储存program的进展，相当于存档
from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter
logger = getLogger(__name__)
logger.setLevel(INFO)
handler1 = StreamHandler()
handler1.setFormatter(Formatter(&quot;%(message)s&quot;))
handler2 = FileHandler(filename=f&quot;&#123;filename&#125;.log&quot;)
handler2.setFormatter(Formatter(&quot;%(message)s&quot;))
logger.addHandler(handler1)
logger.addHandler(handler2)
return logger
</code></pre>
<h3 id="seed-everything-为各种随机数生成器-RNG-设置种子"><a href="#seed-everything-为各种随机数生成器-RNG-设置种子" class="headerlink" title="seed_everything()为各种随机数生成器(RNG)设置种子"></a>seed_everything()为各种随机数生成器(RNG)设置种子</h3><pre><code>def seed_everything(seed=20):
random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
</code></pre>
<h3 id="generate-uuid-创建编号"><a href="#generate-uuid-创建编号" class="headerlink" title="generate_uuid()创建编号"></a>generate_uuid()创建编号</h3><p>def generate_uuid():#创建编号，产生随机且独一无二的uuid<br>    return str(uuid.uuid4())</p>
<h3 id="将tensor-dictionary移动到GPU"><a href="#将tensor-dictionary移动到GPU" class="headerlink" title="将tensor dictionary移动到GPU"></a>将tensor dictionary移动到GPU</h3><p>def to_device(inputs, device: str &#x3D; device):#将tensor dictionary移动到GPU<br>    return {k: v.to(device) for k, v in inputs.items()}</p>
<p>LOGGER &#x3D; get_logger()<br>seed_everything(seed&#x3D;config.SEED)</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(config.MODEL)<br>tokenizer.save_pretrained(paths.OUTPUT_DIR + ‘&#x2F;tokenizer&#x2F;‘)</p>
<h3 id="将原始的文本数据转换为表格"><a href="#将原始的文本数据转换为表格" class="headerlink" title="将原始的文本数据转换为表格"></a>将原始的文本数据转换为表格</h3><p>def prepare_input(cfg: type, text: np.ndarray, tokenizer):</p>
<pre><code>inputs = tokenizer.encode_plus(
    text,
    return_tensors=None,
    add_special_tokens=True,
    max_length=cfg.MAX_LEN,
    padding=&#39;max_length&#39;, # TODO: check padding to max sequence in batch
    truncation=True
)
for k, v in inputs.items():
    inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes
return inputs
</code></pre>
<h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/24/2024-5-24/" data-id="clwkf521j0000bgt05eq4g7mo" data-title="2024-5-24" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MATLAB" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/09/MATLAB/" class="article-date">
  <time class="dt-published" datetime="2024-05-09T06:08:16.000Z" itemprop="datePublished">2024-05-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/09/MATLAB/">MATLAB</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近事情好多哦，我现在还在准备数学建模的期末考试，听说老师不出原题了，会很难吧。。。<br>所以来补习一下MATLAB<br>今天距离考试还有6天，等等，六天？就剩六天了？</p>
<p>基础语法：<br>语句后面有分号：运行后不显示运行结果<br>注释：%或快捷键Ctrl+R,Ctrl+T取消注释<br>clear:清除工作区<br>clc:清除命令行窗口的所有文本<br>这俩经常写在最开头   clear;clc<br>disp(‘  ‘):输出函数<br>input(‘ ‘):输入函数<br>a&#x3D;{1 2 3}:向量<br>合并两个字符串：<br>法一：strcat(‘ ‘,’ ‘)<br>法二：{str1,str2}<br>num2str:将数字转换成字符<br>奇怪，心里莫名好难受，学不进去了。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/09/MATLAB/" data-id="clvyukxh70000rwt04j3x4byy" data-title="MATLAB" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/13/Cursor%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/">Cursor使用教程</a>
          </li>
        
          <li>
            <a href="/2024/12/13/llm%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90-%E6%A1%8C%E5%AE%A0/">llm项目解析-桌宠</a>
          </li>
        
          <li>
            <a href="/2024/12/08/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B0%8F%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90/">大模型小项目解析</a>
          </li>
        
          <li>
            <a href="/2024/08/15/llm%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/">llm训练方法概述</a>
          </li>
        
          <li>
            <a href="/2024/08/01/llm%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">llm应用开发</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>