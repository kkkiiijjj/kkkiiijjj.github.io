<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Forforevery</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Forforevery">
<meta property="og:url" content="https://kkkiiijjj.github.io/index.html">
<meta property="og:site_name" content="Forforevery">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Forforevery" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forforevery</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kkkiiijjj.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-kaggle竞赛——AI-Mathematical" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" class="article-date">
  <time class="dt-published" datetime="2024-06-22T06:41:17.000Z" itemprop="datePublished">2024-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/" data-id="clxpr4tyx00009wt0dmo3hmks" data-title="kaggle竞赛——AI Mathematical" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2024-5-24" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/24/2024-5-24/" class="article-date">
  <time class="dt-published" datetime="2024-05-24T08:27:09.000Z" itemprop="datePublished">2024-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/24/2024-5-24/">2024-5-24</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一些废话：在看kaggle竞赛的课<br>感觉讲的不是很行啊，听半天感觉和没听一样</p>
<p>竞赛：LLM Prompt Recovery | Metric Computation<br>比赛地址直达：<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation">https://www.kaggle.com/code/yeoyunsianggeremie/llm-prompt-recovery-metric-computation</a><br>目的：由原始文本与Gemma改写的文本段落反推提示词(prompt)<br>评价指标：<br>锐化余弦相似度<img src="/image-112.png" alt="alt text"><br>可以用三个不同的模型来解决</p>
<ol>
<li>Seq2Seq</li>
<li>few-shot<br>zero-shot LLM       没有示例<br>one-shot LLM        给一个示例<br>few-shot LLM        给一些示例</li>
<li>Phi 2</li>
</ol>
<p>又是一段废话：<br>然后好像没有有用的话了，code给的还特别小，是想让我瞎吗()<br>好在可以管客服索要代码，看网课不如看代码<br>代码老长老长，给的是.ipynb格式的文件，据说要用juypter打开，不过我用vscode也打开了。<br>觉得自己有必要抽空学一下juypter</p>
<p>代码在这里放一下吧还是</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>%%writefile trian_embedding_generate.py<br>import pandas as pd<br>import gc<br>import numpy as np<br>import pandas as pd<br>import time<br>from tqdm import tqdm<br>import numpy as np<br>from sentence_transformers import SentenceTransformer<br>import pickle</p>
<p>df &#x3D; pd.read_parquet(f”.&#x2F;train_clean.parquet”, columns&#x3D;[‘rewrite_prompt’])</p>
<h4 id="这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下"><a href="#这里用-read-parquet读入-parquet文件，平时比较常用的是pd-read-csv-如下" class="headerlink" title="这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)"></a>这里用.read_parquet读入.parquet文件，平时比较常用的是pd.read_csv(如下)</h4><p>valid &#x3D; pd.read_csv(‘.&#x2F;validation826.csv’, usecols&#x3D;[‘rewrite_prompt’])</p>
<h4 id="两个文件均只导入特定列"><a href="#两个文件均只导入特定列" class="headerlink" title="两个文件均只导入特定列"></a>两个文件均只导入特定列</h4><p>model &#x3D;  SentenceTransformer(‘sentence-transformers&#x2F;sentence-t5-base’)#</p>
<h4 id="选择SentenceTransformer-model进行embedding"><a href="#选择SentenceTransformer-model进行embedding" class="headerlink" title="选择SentenceTransformer model进行embedding"></a>选择SentenceTransformer model进行embedding</h4><p>model.max_seq_length &#x3D; 512</p>
<h4 id="最长序列长度设为512"><a href="#最长序列长度设为512" class="headerlink" title="最长序列长度设为512"></a>最长序列长度设为512</h4><p>encoded_data &#x3D; model.encode(list(df[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>encoded_data &#x3D; encoded_data.detach().cpu().numpy()<br>encoded_data &#x3D; np.asarray(encoded_data.astype(‘float32’))</p>
<p>np.save(‘train_clean_emb_sentence-t5-base.npy’, encoded_data)</p>
<p>valid_emb &#x3D; model.encode(list(valid[‘rewrite_prompt’]), batch_size&#x3D;64, device&#x3D;’cuda’, show_progress_bar&#x3D;True, convert_to_tensor&#x3D;True, normalize_embeddings&#x3D;True)<br>valid_emb &#x3D; valid_emb.detach().cpu().numpy()<br>valid_emb &#x3D; np.asarray(valid_emb.astype(‘float32’))</p>
<p>np.save(‘valid826_emb_sentence-t5-base.npy’, valid_emb)</p>
<h4 id="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里"><a href="#这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826-emb-sentence-t5-base-npy文件里" class="headerlink" title="这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里"></a>这里输入的数据已经被encoded变成numpy数组了，并被保存到valid826_emb_sentence-t5-base.npy文件里</h4><h1 id="seq2seq训练"><a href="#seq2seq训练" class="headerlink" title="seq2seq训练"></a>seq2seq训练</h1><h4 id="the-config-class-provides-a-structured-way-to"><a href="#the-config-class-provides-a-structured-way-to" class="headerlink" title="the config class provides a structured way to"></a>the config class provides a structured way to</h4><h4 id="define-and-manage-the-various-hyperparameters-and-configuration-settings"><a href="#define-and-manage-the-various-hyperparameters-and-configuration-settings" class="headerlink" title="define and manage the various hyperparameters and configuration settings"></a>define and manage the various hyperparameters and configuration settings</h4><h4 id="for-training-the-sequence-to-sequence-model"><a href="#for-training-the-sequence-to-sequence-model" class="headerlink" title="for training the sequence-to-sequence model"></a>for training the sequence-to-sequence model</h4><p>class config:<br>    #### configuration:结构，布局<br>    AMP &#x3D; True  # Boolean flag indicating whether to use Automatic Mixed Precision (AMP) for training efficiency.<br>    BATCH_SIZE_TRAIN &#x3D; 32 #若出现oom，减少即可<br>    BATCH_SIZE_VALID &#x3D; 32 #若出现oom，减少即可<br>    BETAS &#x3D; (0.9, 0.999)<br>    DEBUG &#x3D; 0 #debug改为1<br>    DEVICE &#x3D; torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)<br>    LR &#x3D; 5e-6<br>    EPOCHS &#x3D; 6<br>    EPS &#x3D; 1e-6<br>    GRADIENT_CHECKPOINTING &#x3D; False<br>    MODEL &#x3D; “&#x2F;kaggle&#x2F;input&#x2F;deberta-v3-large-hf-weights” #模型文件-<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights">https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights</a><br>    CKPT &#x3D; ‘deberta-v3-large’<br>    MAX_GRAD_NORM &#x3D; 100000.0<br>    MAX_LEN &#x3D; 384<br>    NUM_WORKERS &#x3D; 0<br>    PRINT_FREQ &#x3D; 500<br>    SEED &#x3D; 20<br>    WANDB &#x3D; False<br>    WEIGHT_DECAY &#x3D; 0.008</p>
<h4 id="提供一个结构化的方法来管理文件路径"><a href="#提供一个结构化的方法来管理文件路径" class="headerlink" title="提供一个结构化的方法来管理文件路径"></a>提供一个结构化的方法来管理文件路径</h4><p>class paths:<br>    TRAIN_DATA &#x3D; “.&#x2F;train_clean.parquet”<br>    #TRAIN_DATA2 &#x3D; ‘.&#x2F;train_sft_v13.csv’<br>    VALID_DATA &#x3D; ‘.&#x2F;validation826.csv’<br>    train_embedding_file &#x3D; ‘.&#x2F;train_clean_emb_sentence-t5-base.npy’<br>    #train_embedding_file2 &#x3D; ‘.&#x2F;train_sft_v13_emb_sentence-t5-base.npy’<br>    valid_embedding_file &#x3D; ‘.&#x2F;valid826_emb_sentence-t5-base.npy’<br>    OUTPUT_DIR &#x3D; “.&#x2F;exp14”#保存文件夹<br>    LOGGER &#x3D; ‘exp14’</p>
<p>os.makedirs(paths.OUTPUT_DIR, exist_ok&#x3D;True)</p>
<h3 id="该类用于在培训或评估过程中监控和跟踪指标的平均值。"><a href="#该类用于在培训或评估过程中监控和跟踪指标的平均值。" class="headerlink" title="该类用于在培训或评估过程中监控和跟踪指标的平均值。"></a>该类用于在培训或评估过程中监控和跟踪指标的平均值。</h3><p>class AverageMeter(object):<br>    def <strong>init</strong>(self):<br>        self.reset()</p>
<pre><code>def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
</code></pre>
<h3 id="该函数将s改为几min几s的形式"><a href="#该函数将s改为几min几s的形式" class="headerlink" title="该函数将s改为几min几s的形式"></a>该函数将s改为几min几s的形式</h3><p>def asMinutes(s):<br>    m &#x3D; math.floor(s &#x2F; 60)<br>    s -&#x3D; m * 60<br>    return ‘%dm %ds’ % (m, s)</p>
<h3 id="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"><a href="#该函数给出已过去的时间以及根据当前进度预计完成任务所需时间" class="headerlink" title="该函数给出已过去的时间以及根据当前进度预计完成任务所需时间"></a>该函数给出已过去的时间以及根据当前进度预计完成任务所需时间</h3><p>def timeSince(since, percent):<br>    now &#x3D; time.time()# 获取当前时间<br>    s &#x3D; now - since# 经过的时间<br>    es &#x3D; s &#x2F; (percent)# es:estimated total time 估计所需总时长<br>    # percent:当前进度<br>    rs &#x3D; es - s# 还要多久完事儿<br>    return ‘%s (remain %s)’ % (asMinutes(s), asMinutes(rs))</p>
<h3 id="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"><a href="#该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用" class="headerlink" title="该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用"></a>该函数创建一个字典，该字典从config类捕获基本参数，更易于管理，以便在训练脚本中进一步使用</h3><p>def get_config_dict(config):<br>    config_dict &#x3D; dict((key, value) for key, value in config.<strong>dict</strong>.items()<br>    if not callable(value) and not key.startswith(‘__’))<br>    return config_dict</p>
<h4 id="该函数为model量身定制learning-rates-和-weight-decay"><a href="#该函数为model量身定制learning-rates-和-weight-decay" class="headerlink" title="该函数为model量身定制learning rates 和 weight decay"></a>该函数为model量身定制learning rates 和 weight decay</h4><p>def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay&#x3D;0.0):<br>    param_optimizer &#x3D; list(model.named_parameters())<br>    no_decay &#x3D; [“bias”, “LayerNorm.bias”, “LayerNorm.weight”]<br>    optimizer_parameters &#x3D; [<br>        {‘params’: [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: weight_decay},<br>        {‘params’: [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],<br>         ‘lr’: encoder_lr, ‘weight_decay’: 0.0},<br>        {‘params’: [p for n, p in model.named_parameters() if “model” not in n],<br>         ‘lr’: decoder_lr, ‘weight_decay’: 0.0}<br>    ]<br>    return optimizer_parameters</p>
<h4 id="在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency"><a href="#在Seq2Seq-模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training-effiency" class="headerlink" title="在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency"></a>在Seq2Seq 模型中，对encoder和decoder使用不同学习率是一种常见的做法，目的是提高training effiency</h4><h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/24/2024-5-24/" data-id="clwkf521j0000bgt05eq4g7mo" data-title="2024-5-24" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MATLAB" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/09/MATLAB/" class="article-date">
  <time class="dt-published" datetime="2024-05-09T06:08:16.000Z" itemprop="datePublished">2024-05-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/09/MATLAB/">MATLAB</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近事情好多哦，我现在还在准备数学建模的期末考试，听说老师不出原题了，会很难吧。。。<br>所以来补习一下MATLAB<br>今天距离考试还有6天，等等，六天？就剩六天了？</p>
<p>基础语法：<br>语句后面有分号：运行后不显示运行结果<br>注释：%或快捷键Ctrl+R,Ctrl+T取消注释<br>clear:清除工作区<br>clc:清除命令行窗口的所有文本<br>这俩经常写在最开头   clear;clc<br>disp(‘  ‘):输出函数<br>input(‘ ‘):输入函数<br>a&#x3D;{1 2 3}:向量<br>合并两个字符串：<br>法一：strcat(‘ ‘,’ ‘)<br>法二：{str1,str2}<br>num2str:将数字转换成字符<br>奇怪，心里莫名好难受，学不进去了。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/05/09/MATLAB/" data-id="clvyukxh70000rwt04j3x4byy" data-title="MATLAB" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-RNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/19/RNN/" class="article-date">
  <time class="dt-published" datetime="2024-04-19T02:26:46.000Z" itemprop="datePublished">2024-04-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/19/RNN/">RNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在看enconder代码的时候发现里面竟然有RNN<br>我还是好好整理一下RNN吧</p>
<p>enconder里的RNN层的作用：<br>实例：（复制chatgpt的，嫌长可以不看）</p>
<ol>
<li>Sentence: “The movie was a disappointment, the acting was terrible.”<br> Processing Words Sequentially: The RNN in the encoder processes the sentence word by word:</li>
</ol>
<p>Word 1: “The”</p>
<p>The RNN receives the embedding vector for “The” and combines it with the initial hidden state (often initialized with zeros).<br>Based on this combination, the RNN updates the hidden state h_1. This initial hidden state doesn’t hold much sentiment information yet.<br>Word 2: “movie”</p>
<p>The RNN receives the embedding vector for “movie” and combines it with the updated hidden state h_1.<br>Now, h_1 contains some context about “The,” which might be neutral. The RNN updates the hidden state h_2 to reflect this context along with the meaning of “movie.”<br>Word 3: “was”</p>
<p>The process repeats. The RNN receives the embedding vector for “was” and combines it with the previous hidden state h_2.<br>h_2 carries information about “The movie,” which could be neutral or slightly positive depending on the embedding vectors. The RNN updates the hidden state h_3 to incorporate the meaning of “was” in this context.<br>Word 4: “a” (similar to “The”)</p>
<p>Word 5: “disappointment”</p>
<p>The RNN receives the embedding vector for “disappointment,” a negative word.<br>The previous hidden state h_4 likely held some neutral or slightly positive sentiment. The RNN updates h_5 to reflect the strong negative connotation of “disappointment,” shifting the overall sentiment towards negative.<br>Words 6-7 follow similarly, likely reinforcing the negative sentiment.</p>
<ol start="3">
<li>Final Hidden State: After processing all words, the RNN in the encoder reaches the final hidden state, let’s call it h_final. This state encapsulates the sentiment of the entire sentence, considering the sequence of words and their relationships.</li>
<li>Using the Hidden State: The h_final can then be used for various tasks:</li>
</ol>
<p>Sentiment Classification: This hidden state can be fed into a classifier (e.g., a simple neural network) to predict the overall sentiment of the sentence (likely negative in this case).<br>Machine Translation: If this is part of a machine translation system, the h_final might be used to encode the meaning of the sentence and generate a corresponding translation in another language.</p>
<p>简而言之，RNN每步读入当前的Word并将其与之前的隐状态结合、更新隐状态，隐状态反映当前输入的所有词之间的relationship<br>在读完整个句子后，我们会得到最后的隐状态，最后的隐状态包含了整句话的所有词之间的relationship<br>最后的隐状态可用于情感分类、机器翻译</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/19/RNN/" data-id="clv61usel0000qgt0gk46hjjd" data-title="RNN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Kaggle-LLM-Science-Exam" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/Kaggle-LLM-Science-Exam/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T06:43:35.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/Kaggle-LLM-Science-Exam/">Kaggle - LLM Science Exam</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>目的：this competition challenges participants to answer difficult science-based questions written by a Large Language Model.<br>任务：预测前三个最可能是正确答案的选项<br>评价标准：Mean Average Precision @ 3 (MAP@3):<br><img src="/image-107.png" alt="alt text"></p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>train集:包括一个问题、五个选项及正确答案<br>test集:是train集的copy,只是没了answer列(啊？这不妥妥过拟合了吗？)</p>
<h2 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h2><h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p>选用Embedding模型：gte-base<br>    还有很多其他Embeding方法，如bge-large-en-v1.5\bge-base-en-v1.5\ember-v1\gte-large\e5-large\bge-small<br>不一定哪个表现最好，视具体情况而定<br>可以拿小样本跑一跑看哪个表现好</p>
<p><img src="/image-108.png" alt="alt text"><br>这里有个问题是若使用检索2的话会使Prompt占的比重很小</p>
<p>一些tricks:若GPU性能没那么好，就调小batch_size(1或2)<br>若改成1还跑不了就调小max_input</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/Kaggle-LLM-Science-Exam/" data-id="clv3g5d5p0000bgt0a3la0wud" data-title="Kaggle - LLM Science Exam" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kaggle—HMS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/17/kaggle%E2%80%94HMS/" class="article-date">
  <time class="dt-published" datetime="2024-04-17T00:57:30.000Z" itemprop="datePublished">2024-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/17/kaggle%E2%80%94HMS/">kaggle—HMS</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>兜兜转转犹犹豫豫最后还是在淘宝上买了kaggle竞赛的课<br>花了我3600大洋呜呜呜呜，不过可以保证我拿银牌，我心里就有底了</p>
<p>那么废话不多说，第一课讲的是HMS</p>
<p>打比赛的流程：<br><img src="/image-100.png" alt="alt text"></p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>比赛目的：检测分类癫痫发作和其他类型的有害大脑活动。<br>比赛任务：开发一个model,根据从医院重症患者记录的脑电图(EEG)信号进行训练。<br>评价指标：KL散度(不常用)</p>
<h2 id="熟悉数据"><a href="#熟悉数据" class="headerlink" title="熟悉数据"></a>熟悉数据</h2><p>data:<br>这数据有啥特征啊。。。</p>
<p><img src="/image-103.png" alt="alt text"><br><img src="/image-101.png" alt="alt text"><br>train数据集包括50秒的EEG(相应匹配于频谱图)<br>[seizure&#x2F;lpd&#x2F;gpd&#x2F;lrda&#x2F;grda&#x2F;other]_vote：<br>人工打分的各类疾病投票<br>一共有6个label，分别是seizure(癫痫)、GRDA、Other、GPD、LRDA、LPD<br>(查不到都是啥病)</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>用librosa包进行特征提取</p>
<h2 id="模型建立"><a href="#模型建立" class="headerlink" title="模型建立"></a>模型建立</h2><p><img src="/image-102.png" alt="alt text"><br><img src="/image-104.png" alt="alt text"><br>方案一直接用给的数据<br><img src="/image-105.png" alt="alt text"><br>方案二用提取特征后的<br><img src="/image-106.png" alt="alt text"><br>方案三用给的+提取特征后的(效果最好)</p>
<p>提分要点：使用vote&gt;10的样本二次训练<br>(即第一次用全部的train集训练，第二次用vote高的进行训练，因为vote高代表准确率更高，但即便如此，vote少的依然可以提供一些信息(钱少也是钱啊))</p>
<p>大致的思路我懂了，但是没怎么看代码</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/17/kaggle%E2%80%94HMS/" data-id="clv33sa2q0000ekt0akf20t8c" data-title="kaggle—HMS" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-prompt" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/13/prompt/" class="article-date">
  <time class="dt-published" datetime="2024-04-13T01:49:55.000Z" itemprop="datePublished">2024-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/13/prompt/">prompt</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>最近在做大创，是关于数字人的，这个项目我很感兴趣，感觉未来可以应用的范围很广，前景也很好。<br>等以后元宇宙火起来了感觉这个会很赚钱</p>
<p>好了回到正题<br><img src="/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240413094011.jpg" alt="alt text"><br>这是我们模型的框架</p>
<h2 id="关于prompt"><a href="#关于prompt" class="headerlink" title="关于prompt:"></a>关于prompt:</h2><p>即提示词(咒语)，是LLM(large language model)的重要起点</p>
<h2 id="RAG检索"><a href="#RAG检索" class="headerlink" title="RAG检索"></a>RAG检索</h2><p>(Retrieval Augmented Generation)<br>结合prompt和其他相关资料，使得LLM的能力更上一层楼<br>RAG可用于回答问题、总结、fact checking、对话系统</p>
<h2 id="多模态融合"><a href="#多模态融合" class="headerlink" title="多模态融合"></a>多模态融合</h2><p>深度学习多模态融合指机器从文本、图像、语音、视频等多个领域获取信息</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/13/prompt/" data-id="cluxfwb0g00009ot0h4bpfvad" data-title="prompt" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LSTM" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/08/LSTM/" class="article-date">
  <time class="dt-published" datetime="2024-04-08T08:44:35.000Z" itemprop="datePublished">2024-04-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/04/08/LSTM/">LSTM</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>是RNN的升级版，用于机器翻译，语音辨识，时间序列预测，文本生成</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>使用门控单元，使得能够长期记忆<br>缓解梯度消失和梯度爆炸</p>
<h2 id="门控单元"><a href="#门控单元" class="headerlink" title="门控单元"></a>门控单元</h2><p>使用门控单元，包括：</p>
<ol>
<li>输入门   数据读入</li>
<li>遗忘门   重置单元内容</li>
<li>输出门   数据输出<br><img src="/image-93.png" alt="alt text"><br>图为这仨门的公式，红色框起的是输入，桔色是参数(b是偏置参数)，黄色是隐状态</li>
</ol>
<h2 id="候选记忆元"><a href="#候选记忆元" class="headerlink" title="候选记忆元"></a>候选记忆元</h2><p>与上面仨门类似，不同点是使用tanh作为激活函数<br><img src="/image-94.png" alt="alt text"><br><img src="/image-95.png" alt="alt text"></p>
<h2 id="记忆元"><a href="#记忆元" class="headerlink" title="记忆元"></a>记忆元</h2><p>公式:<br><img src="/image-96.png" alt="alt text"><br>输入门$I_t$控制采用多少来自$C_t$的新数据， 而遗忘门$F_t$控制保留多少过去的 记忆元$C_{t-1}$的内容。<br><img src="/image-97.png" alt="alt text"></p>
<h2 id="关于隐状态在LSTM中的使用："><a href="#关于隐状态在LSTM中的使用：" class="headerlink" title="关于隐状态在LSTM中的使用："></a>关于隐状态在LSTM中的使用：</h2><p>隐状态将输入的词转换为数字表示，<br>LSTM基于当前的  输入  &amp;  更新的隐状态  来预测下一个单词，如图：<br><img src="/image-99.png" alt="alt text"><br>公式：<img src="/image-98.png" alt="alt text"><br>这咋上来就遗忘呢，为啥不先输入？？</p>
<p>只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p>
<p>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/04/08/LSTM/" data-id="cluqpickq00004gt07w63bw7n" data-title="LSTM" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-房价预测代码解读" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" class="article-date">
  <time class="dt-published" datetime="2024-03-25T06:09:16.000Z" itemprop="datePublished">2024-03-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">房价预测代码解读</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这里解析的是李沐深度学习4.10的代码</p>
<h2 id="导入数据："><a href="#导入数据：" class="headerlink" title="导入数据："></a>导入数据：</h2><pre><code>用pd.read_csv(&quot;&quot;)导入本地数据,()内填写地址
</code></pre>
<p>没什么问题</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><pre><code>将数据的第一列即序号列去掉并将数据集与测试集通过concat()函数连接到一起

这么做的原因是进行standardize和normalize时需用到全部数据,如果分别进行的话会使训练集和测试集的mean和standard deviation不一样，进而影响模型表现
但这么做可能导致leakage(泄露)：比如用训练集的mean去填充测试集的空缺值

是否应该concatenate取决于选择了什么方式填充空缺值
适用concatenate的情况：用所有数据的mean\median\mode(众数)填充
不适用：KNN(K折交叉验证)、插值法、决策树
当不确定是否会造成leakage时，可以总选择分别进行
</code></pre>
<p>这里用的时0填充，所以concat与否无所谓</p>
<p>一行代码需要解释的竟然这么多，读的我好累（</p>
<p>‘’’<br>all_features &#x3D; pd.get_dummies(all_features, dummy_na&#x3D;True)<br>‘’’<br>此行代码将categorical variables转换成dummy variables，即将文字类型转换成数字类型</p>
<p>而后把数据转换为张量并将之前连在一起的数据集和测试集分开</p>
<p>然后定义net,这里用的是线性模型<br>感觉nn.Linear()在这里多余，因为只有一层，但是chatgpt告诉我即使是一层用这个也是有好处的<br>让我不再需要定义forward方法</p>
<p>然后他重定义了loss函数<br>但我不明白为什么不用原来的MSELoss</p>
<h2 id="定义train函数"><a href="#定义train函数" class="headerlink" title="定义train函数"></a>定义train函数</h2><p>这个train函数返回的是两个列表，分别是train和test的loss值<br>这里用了d2l包里的迭代器 分批加载数据<br>然后选择Adam作为优化算法<br>接着是常规操作</p>
<h2 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h2><p>定义函数get_k_fold_data(k, i, X, y)<br>参数：<br>    k:将数据分成k折<br>    i:验证集<br>    X:包含features的张量</p>
<p>返回的值：<br>    X_train:train集 的 features<br>    y_train:train集 的labels<br>    X_valid:validation集 的 features<br>    y_valid:validation集 的 labels</p>
<p>assert k &gt; 1 下断言，保证k&gt;1<br>然后计算fold_size的大小，即每一批有多少数据点<br>循环内将数据分成训练集和验证集</p>
<p>下一个函数def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,<br>           batch_size)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/25/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" data-id="clu6jsnhc0000bst0fcgi18nt" data-title="房价预测代码解读" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-马尔可夫" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" class="article-date">
  <time class="dt-published" datetime="2024-03-24T07:15:03.000Z" itemprop="datePublished">2024-03-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/">马尔可夫</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>关于隐状态：<br>从过去的输入中提取重要特征并用于预测当前或下一个output<br>RNN会一步步更新隐状态，这个更新过程会将 目前的输入 与 去除过去不相关信息后的信息 结合在一起<br>这里举一个经典的例子：有很多盒子。每个盒子里装着不同的球，每次随机从一个盒子里摸出一个球<br>在这个过程中，盒子出现的顺序是隐含状态序列，球出现的顺序是观测序列，隐含状态之间的转换过程是一个马尔可夫过程</p>
<p>GRU(门控单元)<br>包含重置门（reset gate）和更新门（update gate）<br>具体运算：<br><img src="/image-87.png" alt="alt text"><br><img src="/image-88.png" alt="alt text"><br><img src="/image-89.png" alt="alt text"></p>
<p>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</p>
<p>重置门有助于捕获序列中的短期依赖关系。</p>
<p>更新门有助于捕获序列中的长期依赖关系。</p>
<p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p>
<p>自回归模型（autoregressive models）<br>Mechanics：使用线性回归根据过去的数据预测将来，比如天气预报通过分析前几天的温度来预测接下来几天的温度</p>
<p>自回归和马尔可夫的区别：自回归依赖过去的数据，而马尔可夫不，后者仅依赖current state<br>当数据之间存在线性关系时用自回归，短期预测用马尔可夫</p>
<p>HMM(隐马尔可夫)与马尔可夫的区别<br><img src="/image-90.png" alt="alt text"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kkkiiijjj.github.io/2024/03/24/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" data-id="clu56pfe40000sot0322g9ubw" data-title="马尔可夫" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/22/kaggle%E7%AB%9E%E8%B5%9B%E2%80%94%E2%80%94AI-Mathematical/">kaggle竞赛——AI Mathematical</a>
          </li>
        
          <li>
            <a href="/2024/05/24/2024-5-24/">2024-5-24</a>
          </li>
        
          <li>
            <a href="/2024/05/09/MATLAB/">MATLAB</a>
          </li>
        
          <li>
            <a href="/2024/04/19/RNN/">RNN</a>
          </li>
        
          <li>
            <a href="/2024/04/17/Kaggle-LLM-Science-Exam/">Kaggle - LLM Science Exam</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>